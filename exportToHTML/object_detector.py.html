<html>
<head>
<title>object_detector.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
object_detector.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The MediaPipe Authors. All Rights Reserved.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;MediaPipe object detector task.&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">dataclasses</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Mapping</span><span class="s3">, </span><span class="s1">Optional</span>

<span class="s3">from </span><span class="s1">mediapipe.python </span><span class="s3">import </span><span class="s1">packet_creator</span>
<span class="s3">from </span><span class="s1">mediapipe.python </span><span class="s3">import </span><span class="s1">packet_getter</span>
<span class="s3">from </span><span class="s1">mediapipe.python._framework_bindings </span><span class="s3">import </span><span class="s1">image </span><span class="s3">as </span><span class="s1">image_module</span>
<span class="s3">from </span><span class="s1">mediapipe.python._framework_bindings </span><span class="s3">import </span><span class="s1">packet </span><span class="s3">as </span><span class="s1">packet_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.cc.vision.object_detector.proto </span><span class="s3">import </span><span class="s1">object_detector_options_pb2</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.components.containers </span><span class="s3">import </span><span class="s1">detections </span><span class="s3">as </span><span class="s1">detections_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core </span><span class="s3">import </span><span class="s1">base_options </span><span class="s3">as </span><span class="s1">base_options_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core </span><span class="s3">import </span><span class="s1">task_info </span><span class="s3">as </span><span class="s1">task_info_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core.optional_dependencies </span><span class="s3">import </span><span class="s1">doc_controls</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.vision.core </span><span class="s3">import </span><span class="s1">base_vision_task_api</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.vision.core </span><span class="s3">import </span><span class="s1">vision_task_running_mode </span><span class="s3">as </span><span class="s1">running_mode_module</span>

<span class="s1">_BaseOptions = base_options_module.BaseOptions</span>
<span class="s1">_ObjectDetectorOptionsProto = object_detector_options_pb2.ObjectDetectorOptions</span>
<span class="s1">_RunningMode = running_mode_module.VisionTaskRunningMode</span>
<span class="s1">_TaskInfo = task_info_module.TaskInfo</span>

<span class="s1">_DETECTIONS_OUT_STREAM_NAME = </span><span class="s4">'detections_out'</span>
<span class="s1">_DETECTIONS_TAG = </span><span class="s4">'DETECTIONS'</span>
<span class="s1">_IMAGE_IN_STREAM_NAME = </span><span class="s4">'image_in'</span>
<span class="s1">_IMAGE_OUT_STREAM_NAME = </span><span class="s4">'image_out'</span>
<span class="s1">_IMAGE_TAG = </span><span class="s4">'IMAGE'</span>
<span class="s1">_TASK_GRAPH_NAME = </span><span class="s4">'mediapipe.tasks.vision.ObjectDetectorGraph'</span>


<span class="s1">@dataclasses.dataclass</span>
<span class="s3">class </span><span class="s1">ObjectDetectorOptions:</span>
  <span class="s2">&quot;&quot;&quot;Options for the object detector task. 
 
  Attributes: 
    base_options: Base options for the object detector task. 
    running_mode: The running mode of the task. Default to the image mode. 
      Object detector task has three running modes: 
      1) The image mode for detecting objects on single image inputs. 
      2) The video mode for detecting objects on the decoded frames of a video. 
      3) The live stream mode for detecting objects on a live stream of input 
         data, such as from camera. 
    display_names_locale: The locale to use for display names specified through 
      the TFLite Model Metadata. 
    max_results: The maximum number of top-scored classification results to 
      return. 
    score_threshold: Overrides the ones provided in the model metadata. Results 
      below this value are rejected. 
    category_allowlist: Allowlist of category names. If non-empty, detection 
      results whose category name is not in this set will be filtered out. 
      Duplicate or unknown category names are ignored. Mutually exclusive with 
      `category_denylist`. 
    category_denylist: Denylist of category names. If non-empty, detection 
      results whose category name is in this set will be filtered out. Duplicate 
      or unknown category names are ignored. Mutually exclusive with 
      `category_allowlist`. 
    result_callback: The user-defined result callback for processing live stream 
      data. The result callback should only be specified when the running mode 
      is set to the live stream mode. 
  &quot;&quot;&quot;</span>
  <span class="s1">base_options: _BaseOptions</span>
  <span class="s1">running_mode: _RunningMode = _RunningMode.IMAGE</span>
  <span class="s1">display_names_locale: Optional[str] = </span><span class="s3">None</span>
  <span class="s1">max_results: Optional[int] = </span><span class="s3">None</span>
  <span class="s1">score_threshold: Optional[float] = </span><span class="s3">None</span>
  <span class="s1">category_allowlist: Optional[List[str]] = </span><span class="s3">None</span>
  <span class="s1">category_denylist: Optional[List[str]] = </span><span class="s3">None</span>
  <span class="s1">result_callback: Optional[</span>
      <span class="s1">Callable[[detections_module.DetectionResult</span><span class="s3">, </span><span class="s1">image_module.Image</span><span class="s3">, </span><span class="s1">int]</span><span class="s3">,</span>
               <span class="s3">None</span><span class="s1">]] = </span><span class="s3">None</span>

  <span class="s1">@doc_controls.do_not_generate_docs</span>
  <span class="s3">def </span><span class="s1">to_pb2(self) -&gt; _ObjectDetectorOptionsProto:</span>
    <span class="s2">&quot;&quot;&quot;Generates an ObjectDetectorOptions protobuf object.&quot;&quot;&quot;</span>
    <span class="s1">base_options_proto = self.base_options.to_pb2()</span>
    <span class="s1">base_options_proto.use_stream_mode = </span><span class="s3">False if </span><span class="s1">self.running_mode == _RunningMode.IMAGE </span><span class="s3">else True</span>
    <span class="s3">return </span><span class="s1">_ObjectDetectorOptionsProto(</span>
        <span class="s1">base_options=base_options_proto</span><span class="s3">,</span>
        <span class="s1">display_names_locale=self.display_names_locale</span><span class="s3">,</span>
        <span class="s1">max_results=self.max_results</span><span class="s3">,</span>
        <span class="s1">score_threshold=self.score_threshold</span><span class="s3">,</span>
        <span class="s1">category_allowlist=self.category_allowlist</span><span class="s3">,</span>
        <span class="s1">category_denylist=self.category_denylist</span><span class="s3">,</span>
    <span class="s1">)</span>


<span class="s3">class </span><span class="s1">ObjectDetector(base_vision_task_api.BaseVisionTaskApi):</span>
  <span class="s2">&quot;&quot;&quot;Class that performs object detection on images. 
 
  The API expects a TFLite model with mandatory TFLite Model Metadata. 
 
  Input tensor: 
    (kTfLiteUInt8/kTfLiteFloat32) 
    - image input of size `[batch x height x width x channels]`. 
    - batch inference is not supported (`batch` is required to be 1). 
    - only RGB inputs are supported (`channels` is required to be 3). 
    - if type is kTfLiteFloat32, NormalizationOptions are required to be 
      attached to the metadata for input normalization. 
  Output tensors must be the 4 outputs of a `DetectionPostProcess` op, i.e: 
    (kTfLiteFloat32) 
    - locations tensor of size `[num_results x 4]`, the inner array 
      representing bounding boxes in the form [top, left, right, bottom]. 
    - BoundingBoxProperties are required to be attached to the metadata 
      and must specify type=BOUNDARIES and coordinate_type=RATIO. 
    (kTfLiteFloat32) 
    - classes tensor of size `[num_results]`, each value representing the 
      integer index of a class. 
    - optional (but recommended) label map(s) can be attached as 
      AssociatedFile-s with type TENSOR_VALUE_LABELS, containing one label per 
      line. The first such AssociatedFile (if any) is used to fill the 
      `class_name` field of the results. The `display_name` field is filled 
      from the AssociatedFile (if any) whose locale matches the 
      `display_names_locale` field of the `ObjectDetectorOptions` used at 
      creation time (&quot;en&quot; by default, i.e. English). If none of these are 
      available, only the `index` field of the results will be filled. 
    (kTfLiteFloat32) 
    - scores tensor of size `[num_results]`, each value representing the score 
      of the detected object. 
    - optional score calibration can be attached using ScoreCalibrationOptions 
      and an AssociatedFile with type TENSOR_AXIS_SCORE_CALIBRATION. See 
      metadata_schema.fbs [1] for more details. 
    (kTfLiteFloat32) 
    - integer num_results as a tensor of size `[1]` 
 
  An example of such model can be found at: 
  https://tfhub.dev/google/lite-model/object_detection/mobile_object_localizer_v1/1/metadata/1 
 
  [1]: 
  https://github.com/google/mediapipe/blob/6cdc6443b6a7ed662744e2a2ce2d58d9c83e6d6f/mediapipe/tasks/metadata/metadata_schema.fbs#L456 
  &quot;&quot;&quot;</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">create_from_model_path(cls</span><span class="s3">, </span><span class="s1">model_path: str) -&gt; </span><span class="s4">'ObjectDetector'</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Creates an `ObjectDetector` object from a TensorFlow Lite model and the default `ObjectDetectorOptions`. 
 
    Note that the created `ObjectDetector` instance is in image mode, for 
    detecting objects on single image inputs. 
 
    Args: 
      model_path: Path to the model. 
 
    Returns: 
      `ObjectDetector` object that's created from the model file and the default 
      `ObjectDetectorOptions`. 
 
    Raises: 
      ValueError: If failed to create `ObjectDetector` object from the provided 
        file such as invalid file path. 
      RuntimeError: If other types of error occurred. 
    &quot;&quot;&quot;</span>
    <span class="s1">base_options = _BaseOptions(model_asset_path=model_path)</span>
    <span class="s1">options = ObjectDetectorOptions(</span>
        <span class="s1">base_options=base_options</span><span class="s3">, </span><span class="s1">running_mode=_RunningMode.IMAGE)</span>
    <span class="s3">return </span><span class="s1">cls.create_from_options(options)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">create_from_options(cls</span><span class="s3">,</span>
                          <span class="s1">options: ObjectDetectorOptions) -&gt; </span><span class="s4">'ObjectDetector'</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Creates the `ObjectDetector` object from object detector options. 
 
    Args: 
      options: Options for the object detector task. 
 
    Returns: 
      `ObjectDetector` object that's created from `options`. 
 
    Raises: 
      ValueError: If failed to create `ObjectDetector` object from 
        `ObjectDetectorOptions` such as missing the model. 
      RuntimeError: If other types of error occurred. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">packets_callback(output_packets: Mapping[str</span><span class="s3">, </span><span class="s1">packet_module.Packet]):</span>
      <span class="s3">if </span><span class="s1">output_packets[_IMAGE_OUT_STREAM_NAME].is_empty():</span>
        <span class="s3">return</span>
      <span class="s1">detection_proto_list = packet_getter.get_proto_list(</span>
          <span class="s1">output_packets[_DETECTIONS_OUT_STREAM_NAME])</span>
      <span class="s1">detection_result = detections_module.DetectionResult([</span>
          <span class="s1">detections_module.Detection.create_from_pb2(result)</span>
          <span class="s3">for </span><span class="s1">result </span><span class="s3">in </span><span class="s1">detection_proto_list</span>
      <span class="s1">])</span>
      <span class="s1">image = packet_getter.get_image(output_packets[_IMAGE_OUT_STREAM_NAME])</span>
      <span class="s1">timestamp = output_packets[_IMAGE_OUT_STREAM_NAME].timestamp</span>
      <span class="s1">options.result_callback(detection_result</span><span class="s3">, </span><span class="s1">image</span><span class="s3">, </span><span class="s1">timestamp)</span>

    <span class="s1">task_info = _TaskInfo(</span>
        <span class="s1">task_graph=_TASK_GRAPH_NAME</span><span class="s3">,</span>
        <span class="s1">input_streams=[</span><span class="s4">':'</span><span class="s1">.join([_IMAGE_TAG</span><span class="s3">, </span><span class="s1">_IMAGE_IN_STREAM_NAME])]</span><span class="s3">,</span>
        <span class="s1">output_streams=[</span>
            <span class="s4">':'</span><span class="s1">.join([_DETECTIONS_TAG</span><span class="s3">, </span><span class="s1">_DETECTIONS_OUT_STREAM_NAME])</span><span class="s3">,</span>
            <span class="s4">':'</span><span class="s1">.join([_IMAGE_TAG</span><span class="s3">, </span><span class="s1">_IMAGE_OUT_STREAM_NAME])</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s1">task_options=options)</span>
    <span class="s3">return </span><span class="s1">cls(</span>
        <span class="s1">task_info.generate_graph_config(</span>
            <span class="s1">enable_flow_limiting=options.running_mode ==</span>
            <span class="s1">_RunningMode.LIVE_STREAM)</span><span class="s3">, </span><span class="s1">options.running_mode</span><span class="s3">,</span>
        <span class="s1">packets_callback </span><span class="s3">if </span><span class="s1">options.result_callback </span><span class="s3">else None</span><span class="s1">)</span>

  <span class="s0"># TODO: Create an Image class for MediaPipe Tasks.</span>
  <span class="s3">def </span><span class="s1">detect(self</span><span class="s3">,</span>
             <span class="s1">image: image_module.Image) -&gt; detections_module.DetectionResult:</span>
    <span class="s2">&quot;&quot;&quot;Performs object detection on the provided MediaPipe Image. 
 
    Only use this method when the ObjectDetector is created with the image 
    running mode. 
 
    Args: 
      image: MediaPipe Image. 
 
    Returns: 
      A detection result object that contains a list of detections, each 
      detection has a bounding box that is expressed in the unrotated input 
      frame of reference coordinates system, i.e. in `[0,image_width) x [0, 
      image_height)`, which are the dimensions of the underlying image data. 
 
    Raises: 
      ValueError: If any of the input arguments is invalid. 
      RuntimeError: If object detection failed to run. 
    &quot;&quot;&quot;</span>
    <span class="s1">output_packets = self._process_image_data(</span>
        <span class="s1">{_IMAGE_IN_STREAM_NAME: packet_creator.create_image(image)})</span>
    <span class="s1">detection_proto_list = packet_getter.get_proto_list(</span>
        <span class="s1">output_packets[_DETECTIONS_OUT_STREAM_NAME])</span>
    <span class="s3">return </span><span class="s1">detections_module.DetectionResult([</span>
        <span class="s1">detections_module.Detection.create_from_pb2(result)</span>
        <span class="s3">for </span><span class="s1">result </span><span class="s3">in </span><span class="s1">detection_proto_list</span>
    <span class="s1">])</span>

  <span class="s3">def </span><span class="s1">detect_for_video(self</span><span class="s3">, </span><span class="s1">image: image_module.Image</span><span class="s3">,</span>
                       <span class="s1">timestamp_ms: int) -&gt; detections_module.DetectionResult:</span>
    <span class="s2">&quot;&quot;&quot;Performs object detection on the provided video frames. 
 
    Only use this method when the ObjectDetector is created with the video 
    running mode. It's required to provide the video frame's timestamp (in 
    milliseconds) along with the video frame. The input timestamps should be 
    monotonically increasing for adjacent calls of this method. 
 
    Args: 
      image: MediaPipe Image. 
      timestamp_ms: The timestamp of the input video frame in milliseconds. 
 
    Returns: 
      A detection result object that contains a list of detections, each 
      detection has a bounding box that is expressed in the unrotated input 
      frame of reference coordinates system, i.e. in `[0,image_width) x [0, 
      image_height)`, which are the dimensions of the underlying image data. 
 
    Raises: 
      ValueError: If any of the input arguments is invalid. 
      RuntimeError: If object detection failed to run. 
    &quot;&quot;&quot;</span>
    <span class="s1">output_packets = self._process_video_data({</span>
        <span class="s1">_IMAGE_IN_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_image(image).at(timestamp_ms)</span>
    <span class="s1">})</span>
    <span class="s1">detection_proto_list = packet_getter.get_proto_list(</span>
        <span class="s1">output_packets[_DETECTIONS_OUT_STREAM_NAME])</span>
    <span class="s3">return </span><span class="s1">detections_module.DetectionResult([</span>
        <span class="s1">detections_module.Detection.create_from_pb2(result)</span>
        <span class="s3">for </span><span class="s1">result </span><span class="s3">in </span><span class="s1">detection_proto_list</span>
    <span class="s1">])</span>

  <span class="s3">def </span><span class="s1">detect_async(self</span><span class="s3">, </span><span class="s1">image: image_module.Image</span><span class="s3">, </span><span class="s1">timestamp_ms: int) -&gt; </span><span class="s3">None</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Sends live image data (an Image with a unique timestamp) to perform object detection. 
 
    Only use this method when the ObjectDetector is created with the live stream 
    running mode. The input timestamps should be monotonically increasing for 
    adjacent calls of this method. This method will return immediately after the 
    input image is accepted. The results will be available via the 
    `result_callback` provided in the `ObjectDetectorOptions`. The 
    `detect_async` method is designed to process live stream data such as camera 
    input. To lower the overall latency, object detector may drop the input 
    images if needed. In other words, it's not guaranteed to have output per 
    input image. 
 
    The `result_callback` prvoides: 
      - A detection result object that contains a list of detections, each 
        detection has a bounding box that is expressed in the unrotated input 
        frame of reference coordinates system, i.e. in `[0,image_width) x [0, 
        image_height)`, which are the dimensions of the underlying image data. 
      - The input image that the object detector runs on. 
      - The input timestamp in milliseconds. 
 
    Args: 
      image: MediaPipe Image. 
      timestamp_ms: The timestamp of the input image in milliseconds. 
 
    Raises: 
      ValueError: If the current input timestamp is smaller than what the object 
        detector has already processed. 
    &quot;&quot;&quot;</span>
    <span class="s1">self._send_live_stream_data({</span>
        <span class="s1">_IMAGE_IN_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_image(image).at(timestamp_ms)</span>
    <span class="s1">})</span>
</pre>
</body>
</html>