<html>
<head>
<title>gesture_recognizer.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
gesture_recognizer.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The MediaPipe Authors. All Rights Reserved.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;MediaPipe gesture recognizer task.&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">dataclasses</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Mapping</span><span class="s3">, </span><span class="s1">Optional</span><span class="s3">, </span><span class="s1">List</span>

<span class="s3">from </span><span class="s1">mediapipe.framework.formats </span><span class="s3">import </span><span class="s1">classification_pb2</span>
<span class="s3">from </span><span class="s1">mediapipe.framework.formats </span><span class="s3">import </span><span class="s1">landmark_pb2</span>
<span class="s3">from </span><span class="s1">mediapipe.python </span><span class="s3">import </span><span class="s1">packet_creator</span>
<span class="s3">from </span><span class="s1">mediapipe.python </span><span class="s3">import </span><span class="s1">packet_getter</span>
<span class="s3">from </span><span class="s1">mediapipe.python._framework_bindings </span><span class="s3">import </span><span class="s1">image </span><span class="s3">as </span><span class="s1">image_module</span>
<span class="s3">from </span><span class="s1">mediapipe.python._framework_bindings </span><span class="s3">import </span><span class="s1">packet </span><span class="s3">as </span><span class="s1">packet_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.cc.vision.gesture_recognizer.proto </span><span class="s3">import </span><span class="s1">gesture_recognizer_graph_options_pb2</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.components.containers </span><span class="s3">import </span><span class="s1">category </span><span class="s3">as </span><span class="s1">category_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.components.containers </span><span class="s3">import </span><span class="s1">landmark </span><span class="s3">as </span><span class="s1">landmark_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.components.processors </span><span class="s3">import </span><span class="s1">classifier_options</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core </span><span class="s3">import </span><span class="s1">base_options </span><span class="s3">as </span><span class="s1">base_options_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core </span><span class="s3">import </span><span class="s1">task_info </span><span class="s3">as </span><span class="s1">task_info_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core.optional_dependencies </span><span class="s3">import </span><span class="s1">doc_controls</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.vision.core </span><span class="s3">import </span><span class="s1">base_vision_task_api</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.vision.core </span><span class="s3">import </span><span class="s1">image_processing_options </span><span class="s3">as </span><span class="s1">image_processing_options_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.vision.core </span><span class="s3">import </span><span class="s1">vision_task_running_mode </span><span class="s3">as </span><span class="s1">running_mode_module</span>

<span class="s1">_BaseOptions = base_options_module.BaseOptions</span>
<span class="s1">_GestureRecognizerGraphOptionsProto = gesture_recognizer_graph_options_pb2.GestureRecognizerGraphOptions</span>
<span class="s1">_ClassifierOptions = classifier_options.ClassifierOptions</span>
<span class="s1">_RunningMode = running_mode_module.VisionTaskRunningMode</span>
<span class="s1">_ImageProcessingOptions = image_processing_options_module.ImageProcessingOptions</span>
<span class="s1">_TaskInfo = task_info_module.TaskInfo</span>

<span class="s1">_IMAGE_IN_STREAM_NAME = </span><span class="s4">'image_in'</span>
<span class="s1">_IMAGE_OUT_STREAM_NAME = </span><span class="s4">'image_out'</span>
<span class="s1">_IMAGE_TAG = </span><span class="s4">'IMAGE'</span>
<span class="s1">_NORM_RECT_STREAM_NAME = </span><span class="s4">'norm_rect_in'</span>
<span class="s1">_NORM_RECT_TAG = </span><span class="s4">'NORM_RECT'</span>
<span class="s1">_HAND_GESTURE_STREAM_NAME = </span><span class="s4">'hand_gestures'</span>
<span class="s1">_HAND_GESTURE_TAG = </span><span class="s4">'HAND_GESTURES'</span>
<span class="s1">_HANDEDNESS_STREAM_NAME = </span><span class="s4">'handedness'</span>
<span class="s1">_HANDEDNESS_TAG = </span><span class="s4">'HANDEDNESS'</span>
<span class="s1">_HAND_LANDMARKS_STREAM_NAME = </span><span class="s4">'landmarks'</span>
<span class="s1">_HAND_LANDMARKS_TAG = </span><span class="s4">'LANDMARKS'</span>
<span class="s1">_HAND_WORLD_LANDMARKS_STREAM_NAME = </span><span class="s4">'world_landmarks'</span>
<span class="s1">_HAND_WORLD_LANDMARKS_TAG = </span><span class="s4">'WORLD_LANDMARKS'</span>
<span class="s1">_TASK_GRAPH_NAME = </span><span class="s4">'mediapipe.tasks.vision.gesture_recognizer.GestureRecognizerGraph'</span>
<span class="s1">_MICRO_SECONDS_PER_MILLISECOND = </span><span class="s5">1000</span>
<span class="s1">_GESTURE_DEFAULT_INDEX = -</span><span class="s5">1</span>


<span class="s1">@dataclasses.dataclass</span>
<span class="s3">class </span><span class="s1">GestureRecognizerResult:</span>
  <span class="s2">&quot;&quot;&quot;The gesture recognition result from GestureRecognizer, where each vector element represents a single hand detected in the image. 
 
  Attributes: 
    gestures: Recognized hand gestures of detected hands. Note that the index of 
      the gesture is always -1, because the raw indices from multiple gesture 
      classifiers cannot consolidate to a meaningful index. 
    handedness: Classification of handedness. 
    hand_landmarks: Detected hand landmarks in normalized image coordinates. 
    hand_world_landmarks: Detected hand landmarks in world coordinates. 
  &quot;&quot;&quot;</span>

  <span class="s1">gestures: List[List[category_module.Category]]</span>
  <span class="s1">handedness: List[List[category_module.Category]]</span>
  <span class="s1">hand_landmarks: List[List[landmark_module.NormalizedLandmark]]</span>
  <span class="s1">hand_world_landmarks: List[List[landmark_module.Landmark]]</span>


<span class="s3">def </span><span class="s1">_build_recognition_result(</span>
    <span class="s1">output_packets: Mapping[str</span><span class="s3">,</span>
                            <span class="s1">packet_module.Packet]) -&gt; GestureRecognizerResult:</span>
  <span class="s2">&quot;&quot;&quot;Constructs a `GestureRecognizerResult` from output packets.&quot;&quot;&quot;</span>
  <span class="s1">gestures_proto_list = packet_getter.get_proto_list(</span>
      <span class="s1">output_packets[_HAND_GESTURE_STREAM_NAME])</span>
  <span class="s1">handedness_proto_list = packet_getter.get_proto_list(</span>
      <span class="s1">output_packets[_HANDEDNESS_STREAM_NAME])</span>
  <span class="s1">hand_landmarks_proto_list = packet_getter.get_proto_list(</span>
      <span class="s1">output_packets[_HAND_LANDMARKS_STREAM_NAME])</span>
  <span class="s1">hand_world_landmarks_proto_list = packet_getter.get_proto_list(</span>
      <span class="s1">output_packets[_HAND_WORLD_LANDMARKS_STREAM_NAME])</span>

  <span class="s1">gesture_results = []</span>
  <span class="s3">for </span><span class="s1">proto </span><span class="s3">in </span><span class="s1">gestures_proto_list:</span>
    <span class="s1">gesture_categories = []</span>
    <span class="s1">gesture_classifications = classification_pb2.ClassificationList()</span>
    <span class="s1">gesture_classifications.MergeFrom(proto)</span>
    <span class="s3">for </span><span class="s1">gesture </span><span class="s3">in </span><span class="s1">gesture_classifications.classification:</span>
      <span class="s1">gesture_categories.append(</span>
          <span class="s1">category_module.Category(</span>
              <span class="s1">index=_GESTURE_DEFAULT_INDEX</span><span class="s3">,</span>
              <span class="s1">score=gesture.score</span><span class="s3">,</span>
              <span class="s1">display_name=gesture.display_name</span><span class="s3">,</span>
              <span class="s1">category_name=gesture.label))</span>
    <span class="s1">gesture_results.append(gesture_categories)</span>

  <span class="s1">handedness_results = []</span>
  <span class="s3">for </span><span class="s1">proto </span><span class="s3">in </span><span class="s1">handedness_proto_list:</span>
    <span class="s1">handedness_categories = []</span>
    <span class="s1">handedness_classifications = classification_pb2.ClassificationList()</span>
    <span class="s1">handedness_classifications.MergeFrom(proto)</span>
    <span class="s3">for </span><span class="s1">handedness </span><span class="s3">in </span><span class="s1">handedness_classifications.classification:</span>
      <span class="s1">handedness_categories.append(</span>
          <span class="s1">category_module.Category(</span>
              <span class="s1">index=handedness.index</span><span class="s3">,</span>
              <span class="s1">score=handedness.score</span><span class="s3">,</span>
              <span class="s1">display_name=handedness.display_name</span><span class="s3">,</span>
              <span class="s1">category_name=handedness.label))</span>
    <span class="s1">handedness_results.append(handedness_categories)</span>

  <span class="s1">hand_landmarks_results = []</span>
  <span class="s3">for </span><span class="s1">proto </span><span class="s3">in </span><span class="s1">hand_landmarks_proto_list:</span>
    <span class="s1">hand_landmarks = landmark_pb2.NormalizedLandmarkList()</span>
    <span class="s1">hand_landmarks.MergeFrom(proto)</span>
    <span class="s1">hand_landmarks_list = []</span>
    <span class="s3">for </span><span class="s1">hand_landmark </span><span class="s3">in </span><span class="s1">hand_landmarks.landmark:</span>
      <span class="s1">hand_landmarks_list.append(</span>
          <span class="s1">landmark_module.NormalizedLandmark.create_from_pb2(hand_landmark))</span>
    <span class="s1">hand_landmarks_results.append(hand_landmarks_list)</span>

  <span class="s1">hand_world_landmarks_results = []</span>
  <span class="s3">for </span><span class="s1">proto </span><span class="s3">in </span><span class="s1">hand_world_landmarks_proto_list:</span>
    <span class="s1">hand_world_landmarks = landmark_pb2.LandmarkList()</span>
    <span class="s1">hand_world_landmarks.MergeFrom(proto)</span>
    <span class="s1">hand_world_landmarks_list = []</span>
    <span class="s3">for </span><span class="s1">hand_world_landmark </span><span class="s3">in </span><span class="s1">hand_world_landmarks.landmark:</span>
      <span class="s1">hand_world_landmarks_list.append(</span>
          <span class="s1">landmark_module.Landmark.create_from_pb2(hand_world_landmark))</span>
    <span class="s1">hand_world_landmarks_results.append(hand_world_landmarks_list)</span>

  <span class="s3">return </span><span class="s1">GestureRecognizerResult(gesture_results</span><span class="s3">, </span><span class="s1">handedness_results</span><span class="s3">,</span>
                                 <span class="s1">hand_landmarks_results</span><span class="s3">,</span>
                                 <span class="s1">hand_world_landmarks_results)</span>


<span class="s1">@dataclasses.dataclass</span>
<span class="s3">class </span><span class="s1">GestureRecognizerOptions:</span>
  <span class="s2">&quot;&quot;&quot;Options for the gesture recognizer task. 
 
  Attributes: 
    base_options: Base options for the hand gesture recognizer task. 
    running_mode: The running mode of the task. Default to the image mode. 
      Gesture recognizer task has three running modes: 1) The image mode for 
      recognizing hand gestures on single image inputs. 2) The video mode for 
      recognizing hand gestures on the decoded frames of a video. 3) The live 
      stream mode for recognizing hand gestures on a live stream of input data, 
      such as from camera. 
    num_hands: The maximum number of hands can be detected by the recognizer. 
    min_hand_detection_confidence: The minimum confidence score for the hand 
      detection to be considered successful. 
    min_hand_presence_confidence: The minimum confidence score of hand presence 
      score in the hand landmark detection. 
    min_tracking_confidence: The minimum confidence score for the hand tracking 
      to be considered successful. 
    canned_gesture_classifier_options: Options for configuring the canned 
      gestures classifier, such as score threshold, allow list and deny list of 
      gestures. The categories for canned gesture classifiers are: [&quot;None&quot;, 
      &quot;Closed_Fist&quot;, &quot;Open_Palm&quot;, &quot;Pointing_Up&quot;, &quot;Thumb_Down&quot;, &quot;Thumb_Up&quot;, 
      &quot;Victory&quot;, &quot;ILoveYou&quot;]. Note this option is subject to change. 
    custom_gesture_classifier_options: Options for configuring the custom 
      gestures classifier, such as score threshold, allow list and deny list of 
      gestures. Note this option is subject to change. 
    result_callback: The user-defined result callback for processing live stream 
      data. The result callback should only be specified when the running mode 
      is set to the live stream mode. 
  &quot;&quot;&quot;</span>
  <span class="s1">base_options: _BaseOptions</span>
  <span class="s1">running_mode: _RunningMode = _RunningMode.IMAGE</span>
  <span class="s1">num_hands: Optional[int] = </span><span class="s5">1</span>
  <span class="s1">min_hand_detection_confidence: Optional[float] = </span><span class="s5">0.5</span>
  <span class="s1">min_hand_presence_confidence: Optional[float] = </span><span class="s5">0.5</span>
  <span class="s1">min_tracking_confidence: Optional[float] = </span><span class="s5">0.5</span>
  <span class="s1">canned_gesture_classifier_options: Optional[</span>
      <span class="s1">_ClassifierOptions] = dataclasses.field(</span>
          <span class="s1">default_factory=_ClassifierOptions)</span>
  <span class="s1">custom_gesture_classifier_options: Optional[</span>
      <span class="s1">_ClassifierOptions] = dataclasses.field(</span>
          <span class="s1">default_factory=_ClassifierOptions)</span>
  <span class="s1">result_callback: Optional[Callable[</span>
      <span class="s1">[GestureRecognizerResult</span><span class="s3">, </span><span class="s1">image_module.Image</span><span class="s3">, </span><span class="s1">int]</span><span class="s3">, None</span><span class="s1">]] = </span><span class="s3">None</span>

  <span class="s1">@doc_controls.do_not_generate_docs</span>
  <span class="s3">def </span><span class="s1">to_pb2(self) -&gt; _GestureRecognizerGraphOptionsProto:</span>
    <span class="s2">&quot;&quot;&quot;Generates an GestureRecognizerOptions protobuf object.&quot;&quot;&quot;</span>
    <span class="s1">base_options_proto = self.base_options.to_pb2()</span>
    <span class="s1">base_options_proto.use_stream_mode = </span><span class="s3">False if </span><span class="s1">self.running_mode == _RunningMode.IMAGE </span><span class="s3">else True</span>

    <span class="s0"># Initialize gesture recognizer options from base options.</span>
    <span class="s1">gesture_recognizer_options_proto = _GestureRecognizerGraphOptionsProto(</span>
        <span class="s1">base_options=base_options_proto)</span>
    <span class="s0"># Configure hand detector and hand landmarker options.</span>
    <span class="s1">hand_landmarker_options_proto = gesture_recognizer_options_proto.hand_landmarker_graph_options</span>
    <span class="s1">hand_landmarker_options_proto.min_tracking_confidence = self.min_tracking_confidence</span>
    <span class="s1">hand_landmarker_options_proto.hand_detector_graph_options.num_hands = self.num_hands</span>
    <span class="s1">hand_landmarker_options_proto.hand_detector_graph_options.min_detection_confidence = self.min_hand_detection_confidence</span>
    <span class="s1">hand_landmarker_options_proto.hand_landmarks_detector_graph_options.min_detection_confidence = self.min_hand_presence_confidence</span>

    <span class="s0"># Configure hand gesture recognizer options.</span>
    <span class="s1">hand_gesture_recognizer_options_proto = gesture_recognizer_options_proto.hand_gesture_recognizer_graph_options</span>
    <span class="s1">hand_gesture_recognizer_options_proto.canned_gesture_classifier_graph_options.classifier_options.CopyFrom(</span>
        <span class="s1">self.canned_gesture_classifier_options.to_pb2())</span>
    <span class="s1">hand_gesture_recognizer_options_proto.custom_gesture_classifier_graph_options.classifier_options.CopyFrom(</span>
        <span class="s1">self.custom_gesture_classifier_options.to_pb2())</span>

    <span class="s3">return </span><span class="s1">gesture_recognizer_options_proto</span>


<span class="s3">class </span><span class="s1">GestureRecognizer(base_vision_task_api.BaseVisionTaskApi):</span>
  <span class="s2">&quot;&quot;&quot;Class that performs gesture recognition on images.&quot;&quot;&quot;</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">create_from_model_path(cls</span><span class="s3">, </span><span class="s1">model_path: str) -&gt; </span><span class="s4">'GestureRecognizer'</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Creates an `GestureRecognizer` object from a TensorFlow Lite model and the default `GestureRecognizerOptions`. 
 
    Note that the created `GestureRecognizer` instance is in image mode, for 
    recognizing hand gestures on single image inputs. 
 
    Args: 
      model_path: Path to the model. 
 
    Returns: 
      `GestureRecognizer` object that's created from the model file and the 
      default `GestureRecognizerOptions`. 
 
    Raises: 
      ValueError: If failed to create `GestureRecognizer` object from the 
        provided file such as invalid file path. 
      RuntimeError: If other types of error occurred. 
    &quot;&quot;&quot;</span>
    <span class="s1">base_options = _BaseOptions(model_asset_path=model_path)</span>
    <span class="s1">options = GestureRecognizerOptions(</span>
        <span class="s1">base_options=base_options</span><span class="s3">, </span><span class="s1">running_mode=_RunningMode.IMAGE)</span>
    <span class="s3">return </span><span class="s1">cls.create_from_options(options)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">create_from_options(</span>
      <span class="s1">cls</span><span class="s3">, </span><span class="s1">options: GestureRecognizerOptions) -&gt; </span><span class="s4">'GestureRecognizer'</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Creates the `GestureRecognizer` object from gesture recognizer options. 
 
    Args: 
      options: Options for the gesture recognizer task. 
 
    Returns: 
      `GestureRecognizer` object that's created from `options`. 
 
    Raises: 
      ValueError: If failed to create `GestureRecognizer` object from 
        `GestureRecognizerOptions` such as missing the model. 
      RuntimeError: If other types of error occurred. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">packets_callback(output_packets: Mapping[str</span><span class="s3">, </span><span class="s1">packet_module.Packet]):</span>
      <span class="s3">if </span><span class="s1">output_packets[_IMAGE_OUT_STREAM_NAME].is_empty():</span>
        <span class="s3">return</span>

      <span class="s1">image = packet_getter.get_image(output_packets[_IMAGE_OUT_STREAM_NAME])</span>

      <span class="s3">if </span><span class="s1">output_packets[_HAND_GESTURE_STREAM_NAME].is_empty():</span>
        <span class="s1">empty_packet = output_packets[_HAND_GESTURE_STREAM_NAME]</span>
        <span class="s1">options.result_callback(</span>
            <span class="s1">GestureRecognizerResult([]</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">[])</span><span class="s3">, </span><span class="s1">image</span><span class="s3">,</span>
            <span class="s1">empty_packet.timestamp.value // _MICRO_SECONDS_PER_MILLISECOND)</span>
        <span class="s3">return</span>

      <span class="s1">gesture_recognizer_result = _build_recognition_result(output_packets)</span>
      <span class="s1">timestamp = output_packets[_HAND_GESTURE_STREAM_NAME].timestamp</span>
      <span class="s1">options.result_callback(gesture_recognizer_result</span><span class="s3">, </span><span class="s1">image</span><span class="s3">,</span>
                              <span class="s1">timestamp.value // _MICRO_SECONDS_PER_MILLISECOND)</span>

    <span class="s1">task_info = _TaskInfo(</span>
        <span class="s1">task_graph=_TASK_GRAPH_NAME</span><span class="s3">,</span>
        <span class="s1">input_streams=[</span>
            <span class="s4">':'</span><span class="s1">.join([_IMAGE_TAG</span><span class="s3">, </span><span class="s1">_IMAGE_IN_STREAM_NAME])</span><span class="s3">,</span>
            <span class="s4">':'</span><span class="s1">.join([_NORM_RECT_TAG</span><span class="s3">, </span><span class="s1">_NORM_RECT_STREAM_NAME])</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s1">output_streams=[</span>
            <span class="s4">':'</span><span class="s1">.join([_HAND_GESTURE_TAG</span><span class="s3">, </span><span class="s1">_HAND_GESTURE_STREAM_NAME])</span><span class="s3">,</span>
            <span class="s4">':'</span><span class="s1">.join([_HANDEDNESS_TAG</span><span class="s3">, </span><span class="s1">_HANDEDNESS_STREAM_NAME])</span><span class="s3">,</span>
            <span class="s4">':'</span><span class="s1">.join([_HAND_LANDMARKS_TAG</span><span class="s3">,</span>
                      <span class="s1">_HAND_LANDMARKS_STREAM_NAME])</span><span class="s3">, </span><span class="s4">':'</span><span class="s1">.join([</span>
                          <span class="s1">_HAND_WORLD_LANDMARKS_TAG</span><span class="s3">,</span>
                          <span class="s1">_HAND_WORLD_LANDMARKS_STREAM_NAME</span>
                      <span class="s1">])</span><span class="s3">, </span><span class="s4">':'</span><span class="s1">.join([_IMAGE_TAG</span><span class="s3">, </span><span class="s1">_IMAGE_OUT_STREAM_NAME])</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s1">task_options=options)</span>
    <span class="s3">return </span><span class="s1">cls(</span>
        <span class="s1">task_info.generate_graph_config(</span>
            <span class="s1">enable_flow_limiting=options.running_mode ==</span>
            <span class="s1">_RunningMode.LIVE_STREAM)</span><span class="s3">, </span><span class="s1">options.running_mode</span><span class="s3">,</span>
        <span class="s1">packets_callback </span><span class="s3">if </span><span class="s1">options.result_callback </span><span class="s3">else None</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">recognize(</span>
      <span class="s1">self</span><span class="s3">,</span>
      <span class="s1">image: image_module.Image</span><span class="s3">,</span>
      <span class="s1">image_processing_options: Optional[_ImageProcessingOptions] = </span><span class="s3">None</span>
  <span class="s1">) -&gt; GestureRecognizerResult:</span>
    <span class="s2">&quot;&quot;&quot;Performs hand gesture recognition on the given image. 
 
    Only use this method when the GestureRecognizer is created with the image 
    running mode. 
 
    The image can be of any size with format RGB or RGBA. 
    TODO: Describes how the input image will be preprocessed after the yuv 
    support is implemented. 
 
    Args: 
      image: MediaPipe Image. 
      image_processing_options: Options for image processing. 
 
    Returns: 
      The hand gesture recognition results. 
 
    Raises: 
      ValueError: If any of the input arguments is invalid. 
      RuntimeError: If gesture recognition failed to run. 
    &quot;&quot;&quot;</span>
    <span class="s1">normalized_rect = self.convert_to_normalized_rect(</span>
        <span class="s1">image_processing_options</span><span class="s3">, </span><span class="s1">roi_allowed=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">output_packets = self._process_image_data({</span>
        <span class="s1">_IMAGE_IN_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_image(image)</span><span class="s3">,</span>
        <span class="s1">_NORM_RECT_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_proto(normalized_rect.to_pb2())</span>
    <span class="s1">})</span>

    <span class="s3">if </span><span class="s1">output_packets[_HAND_GESTURE_STREAM_NAME].is_empty():</span>
      <span class="s3">return </span><span class="s1">GestureRecognizerResult([]</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">[])</span>

    <span class="s3">return </span><span class="s1">_build_recognition_result(output_packets)</span>

  <span class="s3">def </span><span class="s1">recognize_for_video(</span>
      <span class="s1">self</span><span class="s3">,</span>
      <span class="s1">image: image_module.Image</span><span class="s3">,</span>
      <span class="s1">timestamp_ms: int</span><span class="s3">,</span>
      <span class="s1">image_processing_options: Optional[_ImageProcessingOptions] = </span><span class="s3">None</span>
  <span class="s1">) -&gt; GestureRecognizerResult:</span>
    <span class="s2">&quot;&quot;&quot;Performs gesture recognition on the provided video frame. 
 
    Only use this method when the GestureRecognizer is created with the video 
    running mode. 
 
    Only use this method when the GestureRecognizer is created with the video 
    running mode. It's required to provide the video frame's timestamp (in 
    milliseconds) along with the video frame. The input timestamps should be 
    monotonically increasing for adjacent calls of this method. 
 
    Args: 
      image: MediaPipe Image. 
      timestamp_ms: The timestamp of the input video frame in milliseconds. 
      image_processing_options: Options for image processing. 
 
    Returns: 
      The hand gesture recognition results. 
 
    Raises: 
      ValueError: If any of the input arguments is invalid. 
      RuntimeError: If gesture recognition failed to run. 
    &quot;&quot;&quot;</span>
    <span class="s1">normalized_rect = self.convert_to_normalized_rect(</span>
        <span class="s1">image_processing_options</span><span class="s3">, </span><span class="s1">roi_allowed=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">output_packets = self._process_video_data({</span>
        <span class="s1">_IMAGE_IN_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_image(image).at(</span>
                <span class="s1">timestamp_ms * _MICRO_SECONDS_PER_MILLISECOND)</span><span class="s3">,</span>
        <span class="s1">_NORM_RECT_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_proto(normalized_rect.to_pb2()).at(</span>
                <span class="s1">timestamp_ms * _MICRO_SECONDS_PER_MILLISECOND)</span>
    <span class="s1">})</span>

    <span class="s3">if </span><span class="s1">output_packets[_HAND_GESTURE_STREAM_NAME].is_empty():</span>
      <span class="s3">return </span><span class="s1">GestureRecognizerResult([]</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">[]</span><span class="s3">, </span><span class="s1">[])</span>

    <span class="s3">return </span><span class="s1">_build_recognition_result(output_packets)</span>

  <span class="s3">def </span><span class="s1">recognize_async(</span>
      <span class="s1">self</span><span class="s3">,</span>
      <span class="s1">image: image_module.Image</span><span class="s3">,</span>
      <span class="s1">timestamp_ms: int</span><span class="s3">,</span>
      <span class="s1">image_processing_options: Optional[_ImageProcessingOptions] = </span><span class="s3">None</span>
  <span class="s1">) -&gt; </span><span class="s3">None</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Sends live image data to perform gesture recognition. 
 
    The results will be available via the &quot;result_callback&quot; provided in the 
    GestureRecognizerOptions. Only use this method when the GestureRecognizer 
    is created with the live stream running mode. 
 
    Only use this method when the GestureRecognizer is created with the live 
    stream running mode. The input timestamps should be monotonically increasing 
    for adjacent calls of this method. This method will return immediately after 
    the input image is accepted. The results will be available via the 
    `result_callback` provided in the `GestureRecognizerOptions`. The 
    `recognize_async` method is designed to process live stream data such as 
    camera input. To lower the overall latency, gesture recognizer may drop the 
    input images if needed. In other words, it's not guaranteed to have output 
    per input image. 
 
    The `result_callback` provides: 
      - The hand gesture recognition results. 
      - The input image that the gesture recognizer runs on. 
      - The input timestamp in milliseconds. 
 
    Args: 
      image: MediaPipe Image. 
      timestamp_ms: The timestamp of the input image in milliseconds. 
      image_processing_options: Options for image processing. 
 
    Raises: 
      ValueError: If the current input timestamp is smaller than what the 
      gesture recognizer has already processed. 
    &quot;&quot;&quot;</span>
    <span class="s1">normalized_rect = self.convert_to_normalized_rect(</span>
        <span class="s1">image_processing_options</span><span class="s3">, </span><span class="s1">roi_allowed=</span><span class="s3">False</span><span class="s1">)</span>
    <span class="s1">self._send_live_stream_data({</span>
        <span class="s1">_IMAGE_IN_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_image(image).at(</span>
                <span class="s1">timestamp_ms * _MICRO_SECONDS_PER_MILLISECOND)</span><span class="s3">,</span>
        <span class="s1">_NORM_RECT_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_proto(normalized_rect.to_pb2()).at(</span>
                <span class="s1">timestamp_ms * _MICRO_SECONDS_PER_MILLISECOND)</span>
    <span class="s1">})</span>
</pre>
</body>
</html>