<html>
<head>
<title>metadata_schema.fbs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #a9b7c6;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
metadata_schema.fbs</font>
</center></td></tr></table>
<pre><span class="s0">// Copyright 2022 The MediaPipe Authors. All Rights Reserved.</span>
<span class="s0">//</span>
<span class="s0">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0">// you may not use this file except in compliance with the License.</span>
<span class="s0">// You may obtain a copy of the License at</span>
<span class="s0">//</span>
<span class="s0">//     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">//</span>
<span class="s0">// Unless required by applicable law or agreed to in writing, software</span>
<span class="s0">// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0">// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0">// See the License for the specific language governing permissions and</span>
<span class="s0">// limitations under the License.</span>

<span class="s0">namespace tflite;</span>

<span class="s0">// TFLite metadata contains both human readable and machine readable information</span>
<span class="s0">// about what the model does and how to use the model. It can be used as a</span>
<span class="s0">// README file, which elaborates the details of the model, each input/ouput</span>
<span class="s0">// tensor, and each associated file.</span>
<span class="s0">//</span>
<span class="s0">// An important use case of TFLite metadata is the TFLite codegen tool, which</span>
<span class="s0">// automatically generates the model interface based on the properties of the</span>
<span class="s0">// model and the tensors. The model interface provides high-level APIs to</span>
<span class="s0">// interact with the model, such as preprocessing the input data and running</span>
<span class="s0">// inferences.</span>
<span class="s0">//</span>
<span class="s0">// Entries marked with &quot;&lt;Codegen usage&gt;&quot; are used in TFLite codegen tool to</span>
<span class="s0">// generate the model interface. It is recommended to fill in at least those</span>
<span class="s0">// enties to boost the codegen performance.</span>

<span class="s0">// The Metadata schema is versioned by the Semantic versioning number, such as</span>
<span class="s0">// MAJOR.MINOR.PATCH. It tracks the schema changes according to the rules below:</span>
<span class="s0">//  * Bump up the MAJOR number when making potentially backwards incompatible</span>
<span class="s0">//    changes. It must be incremented if the new changes break the backwards</span>
<span class="s0">//    compatibility. It may also include minor and patch level changes as</span>
<span class="s0">//    needed. The true backwards compatibility is indicated by the file</span>
<span class="s0">//    identifier.</span>
<span class="s0">//  * Bump up the MINOR number when making backwards compatible updates for</span>
<span class="s0">//    major features, such as supporting new content types or adding new</span>
<span class="s0">//    processing units.</span>
<span class="s0">//  * Bump up the PATCH number when making small backwards compatible changes,</span>
<span class="s0">//    such as adding a new fields or deprecating certain fields (not deleting</span>
<span class="s0">//    them).</span>
<span class="s0">//</span>
<span class="s0">// ModelMetadata.min_parser_version indicates the minimum necessary metadata</span>
<span class="s0">// parser version to fully understand all fields in a given metadata flatbuffer.</span>
<span class="s0">//</span>
<span class="s0">// New fields and types will have associated comments with the schema version</span>
<span class="s0">// for which they were added.</span>
<span class="s0">//</span>
<span class="s0">// TODO: Add LINT change check as needed.</span>
<span class="s0">// Schema Semantic version: 1.4.1</span>

<span class="s0">// This indicates the flatbuffer compatibility. The number will bump up when a</span>
<span class="s0">// break change is applied to the schema, such as removing fields or adding new</span>
<span class="s0">// fields to the middle of a table.</span>
<span class="s0">file_identifier &quot;M001&quot;;</span>

<span class="s0">// History:</span>
<span class="s0">// 1.0.1 - Added VOCABULARY type to AssociatedFileType.</span>
<span class="s0">// 1.1.0 - Added BertTokenizerOptions to ProcessUnitOptions.</span>
<span class="s0">//         Added SentencePieceTokenizerOptions to ProcessUnitOptions.</span>
<span class="s0">//         Added input_process_units to SubGraphMetadata.</span>
<span class="s0">//         Added output_process_units to SubGraphMetadata.</span>
<span class="s0">// 1.2.0 - Added input_tensor_group to SubGraphMetadata.</span>
<span class="s0">//         Added output_tensor_group to SubGraphMetadata.</span>
<span class="s0">// 1.2.1 - Added RegexTokenizerOptions to ProcessUnitOptions.</span>
<span class="s0">// 1.3.0 - Added AudioProperties to ContentProperties.</span>
<span class="s0">// 1.4.0 - Added SCANN_INDEX_FILE type to AssociatedFileType.</span>
<span class="s0">// 1.4.1 - Added version to AssociatedFile.</span>

<span class="s0">// File extension of any written files.</span>
<span class="s0">file_extension &quot;tflitemeta&quot;;</span>

<span class="s0">// TODO: Add LINT change check as needed.</span>
<span class="s0">enum AssociatedFileType : byte {</span>
  <span class="s0">UNKNOWN = 0,</span>

  <span class="s0">// Files such as readme.txt.</span>
  <span class="s0">DESCRIPTIONS = 1,</span>

  <span class="s0">// Contains a list of labels (characters separated by &quot;\n&quot; or in lines) that</span>
  <span class="s0">// annotate certain axis of the tensor. For example,</span>
  <span class="s0">// the label file in image classification. Those labels annotate the</span>
  <span class="s0">// the output tensor, such that each value in the output tensor is the</span>
  <span class="s0">// probability of that corresponding category specified by the label. See the</span>
  <span class="s0">// example label file used in image classification [1].</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// If an output tensor has an associated file as TENSOR_AXIS_LABELS, return</span>
  <span class="s0">// the output as a mapping between the labels and probability in the model</span>
  <span class="s0">// interface.</span>
  <span class="s0">// If multiple files of the same type are present, the first one is used by</span>
  <span class="s0">// default; additional ones are to be distinguished from one another by their</span>
  <span class="s0">// specified locale.</span>
  <span class="s0">//</span>
  <span class="s0">// TODO: Add github example link.</span>
  <span class="s0">TENSOR_AXIS_LABELS = 2,</span>

  <span class="s0">// Contains a list of labels (characters separated by &quot;\n&quot; or in lines) that</span>
  <span class="s0">// tensor values correspond to. For example, in</span>
  <span class="s0">// the object detection model, one of the output tensors is the detected</span>
  <span class="s0">// classes. And each value in the tensor refers to the index of label in the</span>
  <span class="s0">// category label file. See the example label file used in object detection</span>
  <span class="s0">// [1].</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// If an output tensor has an associated file as TENSOR_VALUE_LABELS, convert</span>
  <span class="s0">// the tensor values into labels, and return a list of string as the output.</span>
  <span class="s0">// If multiple files of the same type are present, the first one is used by</span>
  <span class="s0">// default; additional ones are to be distinguished from one another by their</span>
  <span class="s0">// specified locale.</span>
  <span class="s0">//</span>
  <span class="s0">// TODO: Add github example link.</span>
  <span class="s0">TENSOR_VALUE_LABELS = 3,</span>

  <span class="s0">// Contains sigmoid-based score calibration parameters, formatted as CSV.</span>
  <span class="s0">// Lines contain for each index of an output tensor the scale, slope, offset</span>
  <span class="s0">// and (optional) min_score parameters to be used for sigmoid fitting (in this</span>
  <span class="s0">// order and in `strtof`-compatible [1] format). Scale should be a</span>
  <span class="s0">// non-negative value.</span>
  <span class="s0">// A line may be left empty to default calibrated scores for this index to</span>
  <span class="s0">// default_score.</span>
  <span class="s0">// In summary, each line should thus contain 0, 3 or 4 comma-separated values.</span>
  <span class="s0">//</span>
  <span class="s0">// See the example score calibration file used in image classification [2].</span>
  <span class="s0">//</span>
  <span class="s0">// See documentation for ScoreCalibrationOptions for details.</span>
  <span class="s0">//</span>
  <span class="s0">// [1]: https://en.cppreference.com/w/c/string/byte/strtof</span>
  <span class="s0">// TODO: Add github example link.</span>
  <span class="s0">TENSOR_AXIS_SCORE_CALIBRATION = 4,</span>

  <span class="s0">// Contains a list of unique words (characters separated by &quot;\n&quot; or in lines)</span>
  <span class="s0">// that help to convert natural language words to embedding vectors.</span>
  <span class="s0">//</span>
  <span class="s0">// See the example vocab file used in text classification [1].</span>
  <span class="s0">//</span>
  <span class="s0">// TODO: Add github example link.</span>
  <span class="s0">// Added in: 1.0.1</span>
  <span class="s0">VOCABULARY = 5,</span>

  <span class="s0">// TODO: introduce the ScaNN index file with links once the code</span>
  <span class="s0">// is released.</span>
  <span class="s0">// Contains on-devide ScaNN index file with LevelDB format.</span>
  <span class="s0">// Added in: 1.4.0</span>
  <span class="s0">SCANN_INDEX_FILE = 6,</span>
<span class="s0">}</span>

<span class="s0">table AssociatedFile {</span>
  <span class="s0">// Name of this file. Need to be exact the same as the name of the actual file</span>
  <span class="s0">// packed into the TFLite model as a zip file.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Locates to the actual file in the TFLite model.</span>
  <span class="s0">name:string;</span>

  <span class="s0">// A description of what the file is.</span>
  <span class="s0">description:string;</span>

  <span class="s0">// Type of the associated file. There may be special pre/post processing for</span>
  <span class="s0">// some types. For example in image classification, a label file of the output</span>
  <span class="s0">// will be used to convert object index into string.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Determines how to process the corresponding tensor.</span>
  <span class="s0">type:AssociatedFileType;</span>

  <span class="s0">// An optional locale for this associated file (if applicable). It is</span>
  <span class="s0">// recommended to use an ISO 639-1 letter code (e.g. &quot;en&quot; for English),</span>
  <span class="s0">// optionally completed by a two letter region code (e.g. &quot;en-US&quot; for US</span>
  <span class="s0">// English and &quot;en-CA&quot; for Canadian English).</span>
  <span class="s0">// Leverage this in order to specify e.g multiple label files translated in</span>
  <span class="s0">// different languages.</span>
  <span class="s0">locale:string;</span>

  <span class="s0">// Version of the file specified by model creators.</span>
  <span class="s0">// Added in: 1.4.1</span>
  <span class="s0">version:string;</span>
<span class="s0">}</span>

<span class="s0">// The basic content type for all tensors.</span>
<span class="s0">//</span>
<span class="s0">// &lt;Codegen usage&gt;:</span>
<span class="s0">// Input feature tensors:</span>
<span class="s0">// 1. Generates the method to load data from a TensorBuffer.</span>
<span class="s0">// 2. Creates the preprocessing logic. The default processing pipeline is:</span>
<span class="s0">// [NormalizeOp, QuantizeOp].</span>
<span class="s0">// Output feature tensors:</span>
<span class="s0">// 1. Generates the method to return the output data to a TensorBuffer.</span>
<span class="s0">// 2. Creates the post-processing logic. The default processing pipeline is:</span>
<span class="s0">// [DeQuantizeOp].</span>
<span class="s0">table FeatureProperties {</span>
<span class="s0">}</span>

<span class="s0">// The type of color space of an image.</span>
<span class="s0">enum ColorSpaceType : byte {</span>
  <span class="s0">UNKNOWN = 0,</span>
  <span class="s0">RGB = 1,</span>
  <span class="s0">GRAYSCALE = 2,</span>
<span class="s0">}</span>

<span class="s0">table ImageSize {</span>
  <span class="s0">width:uint;</span>
  <span class="s0">height:uint;</span>
<span class="s0">}</span>

<span class="s0">// The properties for image tensors.</span>
<span class="s0">//</span>
<span class="s0">// &lt;Codegen usage&gt;:</span>
<span class="s0">// Input image tensors:</span>
<span class="s0">// 1. Generates the method to load an image from a TensorImage.</span>
<span class="s0">// 2. Creates the preprocessing logic. The default processing pipeline is:</span>
<span class="s0">// [ResizeOp, NormalizeOp, QuantizeOp].</span>
<span class="s0">// Output image tensors:</span>
<span class="s0">// 1. Generates the method to return the output data to a TensorImage.</span>
<span class="s0">// 2. Creates the post-processing logic. The default processing pipeline is:</span>
<span class="s0">// [DeQuantizeOp].</span>
<span class="s0">table ImageProperties {</span>
  <span class="s0">// The color space of the image.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Determines how to convert the color space of a given image from users.</span>
  <span class="s0">color_space:ColorSpaceType;</span>

  <span class="s0">// Indicates the default value of image width and height if the tensor shape</span>
  <span class="s0">// is dynamic. For fixed-size tensor, this size will be consistent with the</span>
  <span class="s0">// expected size.</span>
  <span class="s0">default_size:ImageSize;</span>
<span class="s0">}</span>

<span class="s0">// The properties for tensors representing bounding boxes.</span>
<span class="s0">//</span>
<span class="s0">// &lt;Codegen usage&gt;:</span>
<span class="s0">// Input image tensors: NA.</span>
<span class="s0">// Output image tensors: parses the values into a data stucture that represents</span>
<span class="s0">// bounding boxes. For example, in the generated wrapper for Android, it returns</span>
<span class="s0">// the output as android.graphics.Rect objects.</span>
<span class="s0">enum BoundingBoxType : byte {</span>
  <span class="s0">UNKNOWN = 0,</span>
  <span class="s0">// Represents the bounding box by using the combination of boundaries,</span>
  <span class="s0">// {left, top, right, bottom}.</span>
  <span class="s0">// The default order is {left, top, right, bottom}. Other orders can be</span>
  <span class="s0">// indicated by BoundingBoxProperties.index.</span>
  <span class="s0">BOUNDARIES = 1,</span>

  <span class="s0">// Represents the bounding box by using the upper_left corner, width and</span>
  <span class="s0">// height.</span>
  <span class="s0">// The default order is {upper_left_x, upper_left_y, width, height}. Other</span>
  <span class="s0">// orders can be indicated by BoundingBoxProperties.index.</span>
  <span class="s0">UPPER_LEFT = 2,</span>

  <span class="s0">// Represents the bounding box by using the center of the box, width and</span>
  <span class="s0">// height. The default order is {center_x, center_y, width, height}. Other</span>
  <span class="s0">// orders can be indicated by BoundingBoxProperties.index.</span>
  <span class="s0">CENTER = 3,</span>

<span class="s0">}</span>

<span class="s0">// The properties for audio tensors.</span>
<span class="s0">// Added in: 1.3.0</span>
<span class="s0">table AudioProperties {</span>
  <span class="s0">// The sample rate in Hz when the audio was captured.</span>
  <span class="s0">sample_rate:uint;</span>

  <span class="s0">// The channel count of the audio.</span>
  <span class="s0">channels:uint;</span>
<span class="s0">}</span>

<span class="s0">enum CoordinateType : byte {</span>
  <span class="s0">// The coordinates are float values from 0 to 1.</span>
  <span class="s0">RATIO = 0,</span>
  <span class="s0">// The coordinates are integers.</span>
  <span class="s0">PIXEL = 1,</span>
<span class="s0">}</span>

<span class="s0">table BoundingBoxProperties {</span>
  <span class="s0">// Denotes the order of the elements defined in each bounding box type. An</span>
  <span class="s0">// empty index array represent the default order of each bounding box type.</span>
  <span class="s0">// For example, to denote the default order of BOUNDARIES, {left, top, right,</span>
  <span class="s0">// bottom}, the index should be {0, 1, 2, 3}. To denote the order {left,</span>
  <span class="s0">// right, top, bottom}, the order should be {0, 2, 1, 3}.</span>
  <span class="s0">//</span>
  <span class="s0">// The index array can be applied to all bounding box types to adjust the</span>
  <span class="s0">// order of their corresponding underlying elements.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Indicates how to parse the bounding box values.</span>
  <span class="s0">index:[uint];</span>

  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Indicates how to parse the bounding box values.</span>
  <span class="s0">type:BoundingBoxType;</span>

  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Indicates how to convert the bounding box back to the original image in</span>
  <span class="s0">// pixels.</span>
  <span class="s0">coordinate_type:CoordinateType;</span>
<span class="s0">}</span>

<span class="s0">union ContentProperties {</span>
  <span class="s0">FeatureProperties,</span>
  <span class="s0">ImageProperties,</span>
  <span class="s0">BoundingBoxProperties,</span>
  <span class="s0">// Added in: 1.3.0</span>
  <span class="s0">AudioProperties,</span>
<span class="s0">}</span>

<span class="s0">table ValueRange {</span>
  <span class="s0">min:int;</span>
  <span class="s0">max:int;</span>
<span class="s0">}</span>

<span class="s0">table Content {</span>
  <span class="s0">// The properties that the content may have, indicating the type of the</span>
  <span class="s0">// Content.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Indicates how to process the tensor.</span>
  <span class="s0">content_properties:ContentProperties;</span>

  <span class="s0">// The range of dimensions that the content corresponds to. A NULL</span>
  <span class="s0">// &quot;range&quot; indicates that the content uses up all dimensions,</span>
  <span class="s0">// except the batch axis if applied.</span>
  <span class="s0">//</span>
  <span class="s0">// Here are all the possible situations of how a tensor is composed.</span>
  <span class="s0">// Case 1: The tensor is a single object, such as an image.</span>
  <span class="s0">// For example, the input of an image classifier</span>
  <span class="s0">// (https://www.tensorflow.org/lite/models/image_classification/overview),</span>
  <span class="s0">// a tensor of shape [1, 224, 224, 3]. Dimensions 1 to 3 correspond to the</span>
  <span class="s0">// image. Since dimension 0 is a batch axis, which can be ignored,</span>
  <span class="s0">// &quot;range&quot; can be left as NULL.</span>
  <span class="s0">//</span>
  <span class="s0">// Case 2: The tensor contains multiple instances of the same object.</span>
  <span class="s0">// For example, the output tensor of detected bounding boxes of an object</span>
  <span class="s0">// detection model</span>
  <span class="s0">// (https://www.tensorflow.org/lite/models/object_detection/overview).</span>
  <span class="s0">// The tensor shape is [1, 10, 4]. Here is the what the three dimensions</span>
  <span class="s0">// represent for:</span>
  <span class="s0">// dimension 0: the batch axis.</span>
  <span class="s0">// dimension 1: the 10 objects detected with the highest confidence.</span>
  <span class="s0">// dimension 2: the bounding boxes of the 10 detected objects.</span>
  <span class="s0">// The tensor is essentially 10 bounding boxes. In this case,</span>
  <span class="s0">// &quot;range&quot; should be {min=2; max=2;}.</span>
  <span class="s0">//</span>
  <span class="s0">// The output tensor of scores of the above object detection model has shape</span>
  <span class="s0">// [1, 10], where</span>
  <span class="s0">// dimension 0: the batch axis;</span>
  <span class="s0">// dimension 1: the scores of the 10 detected objects.</span>
  <span class="s0">// Set &quot;range&quot; to the number of dimensions which is {min=2; max=2;} to denote</span>
  <span class="s0">// that every element in the tensor is an individual content object, i.e. a</span>
  <span class="s0">// score in this example.</span>
  <span class="s0">//</span>
  <span class="s0">// Another example is the pose estimation model</span>
  <span class="s0">// (https://www.tensorflow.org/lite/models/pose_estimation/overview).</span>
  <span class="s0">// The output tensor of heatmaps is in the shape of [1, 9, 9, 17].</span>
  <span class="s0">// Here is the what the four dimensions represent for:</span>
  <span class="s0">// dimension 0: the batch axis.</span>
  <span class="s0">// dimension 1/2: the heatmap image.</span>
  <span class="s0">// dimension 3: 17 body parts of a person.</span>
  <span class="s0">// Even though the last axis is body part, the real content of this tensor is</span>
  <span class="s0">// the heatmap. &quot;range&quot; should be [min=1; max=2].</span>
  <span class="s0">//</span>
  <span class="s0">// Case 3: The tensor contains multiple different objects. (Not supported by</span>
  <span class="s0">// Content at this point).</span>
  <span class="s0">// Sometimes a tensor may contain multiple different objects, thus different</span>
  <span class="s0">// contents. It is very common for regression models. For example, a model</span>
  <span class="s0">// to predict the fuel efficiency</span>
  <span class="s0">// (https://www.tensorflow.org/tutorials/keras/regression).</span>
  <span class="s0">// The input tensor has shape [1, 9], consisting of 9 features, such as</span>
  <span class="s0">// &quot;Cylinders&quot;, &quot;Displacement&quot;, &quot;Weight&quot;, etc. In this case, dimension 1</span>
  <span class="s0">// contains 9 different contents. However, since these sub-dimension objects</span>
  <span class="s0">// barely need to be specifically processed, their contents are not recorded</span>
  <span class="s0">// in the metadata. Through, the name of each dimension can be set through</span>
  <span class="s0">// TensorMetadata.dimension_names.</span>
  <span class="s0">//</span>
  <span class="s0">// Note that if it is not case 3, a tensor can only have one content type.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Case 1: return a processed single object of certain content type.</span>
  <span class="s0">// Case 2: return a list of processed objects of certain content type. The</span>
  <span class="s0">// generated model interface have API to random access those objects from</span>
  <span class="s0">// the output.</span>
  <span class="s0">range:ValueRange;</span>
<span class="s0">}</span>

<span class="s0">// Parameters that are used when normalizing the tensor.</span>
<span class="s0">table NormalizationOptions{</span>
  <span class="s0">// mean and std are normalization parameters. Tensor values are normalized</span>
  <span class="s0">// on a per-channel basis, by the formula</span>
  <span class="s0">//   (x - mean) / std.</span>
  <span class="s0">// If there is only one value in mean or std, we'll propogate the value to</span>
  <span class="s0">// all channels.</span>
  <span class="s0">//</span>
  <span class="s0">// Quantized models share the same normalization parameters as their</span>
  <span class="s0">// corresponding float models. For example, an image input tensor may have</span>
  <span class="s0">// the normalization parameter of</span>
  <span class="s0">//   mean = 127.5f and std = 127.5f.</span>
  <span class="s0">// The image value will be normalized from [0, 255] to [-1, 1].</span>
  <span class="s0">// Then, for quantized models, the image data should be further quantized</span>
  <span class="s0">// according to the quantization parameters. In the case of uint8, the image</span>
  <span class="s0">// data will be scaled back to [0, 255], while for int8, the image data will</span>
  <span class="s0">// be scaled to [-128, 127].</span>
  <span class="s0">//</span>
  <span class="s0">// Both the normalization parameters and quantization parameters can be</span>
  <span class="s0">// retrieved through the metadata extractor library.</span>
  <span class="s0">// TODO: add link for the metadata extractor library.</span>

  <span class="s0">// Per-channel mean of the possible values used in normalization.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Apply normalization to input tensors accordingly.</span>
  <span class="s0">mean:[float];</span>

  <span class="s0">// Per-channel standard dev. of the possible values used in normalization.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Apply normalization to input tensors accordingly.</span>
  <span class="s0">std:[float];</span>
<span class="s0">}</span>

<span class="s0">// The different possible score transforms to apply to uncalibrated scores</span>
<span class="s0">// before applying score calibration.</span>
<span class="s0">enum ScoreTransformationType : byte {</span>
  <span class="s0">// Identity function: g(x) = x.</span>
  <span class="s0">IDENTITY = 0,</span>
  <span class="s0">// Log function: g(x) = log(x).</span>
  <span class="s0">LOG = 1,</span>
  <span class="s0">// Inverse logistic function: g(x) = log(x) - log(1-x).</span>
  <span class="s0">INVERSE_LOGISTIC = 2,</span>
<span class="s0">}</span>

<span class="s0">// Options to perform score calibration on an output tensor through sigmoid</span>
<span class="s0">// functions. One of the main purposes of score calibration is to make scores</span>
<span class="s0">// across classes comparable, so that a common threshold can be used for all</span>
<span class="s0">// output classes. This is meant for models producing class predictions as</span>
<span class="s0">// output, e.g. image classification or detection models.</span>
<span class="s0">//</span>
<span class="s0">// For each index in the output tensor, this applies:</span>
<span class="s0">// * `f(x) = scale / (1 + e^-(slope*g(x)+offset))` if `x &gt; min_score` or if no</span>
<span class="s0">//   `min_score` has been specified,</span>
<span class="s0">// * `f(x) = default_score` otherwise or if no scale, slope and offset have been</span>
<span class="s0">//   specified.</span>
<span class="s0">// Where:</span>
<span class="s0">// * scale, slope, offset and (optional) min_score are index-specific parameters</span>
<span class="s0">// * g(x) is an index-independent transform among those defined in</span>
<span class="s0">//   ScoreTransformationType</span>
<span class="s0">// * default_score is an index-independent parameter.</span>
<span class="s0">// An AssociatedFile with type TANSOR_AXIS_SCORE_CALIBRATION specifying the</span>
<span class="s0">// index-specific parameters must be associated with the corresponding</span>
<span class="s0">// TensorMetadata for score calibration be applied.</span>
<span class="s0">//</span>
<span class="s0">// See the example score calibration file used in image classification [1].</span>
<span class="s0">// TODO: Add github example link.</span>
<span class="s0">table ScoreCalibrationOptions {</span>
  <span class="s0">// The function to use for transforming the uncalibrated score before</span>
  <span class="s0">// applying score calibration.</span>
  <span class="s0">score_transformation:ScoreTransformationType;</span>

  <span class="s0">// The default calibrated score to apply if the uncalibrated score is</span>
  <span class="s0">// below min_score or if no parameters were specified for a given index.</span>
  <span class="s0">default_score:float;</span>
<span class="s0">}</span>

<span class="s0">// Performs thresholding on output tensor values, in order to filter out</span>
<span class="s0">// low-confidence results.</span>
<span class="s0">table ScoreThresholdingOptions {</span>
  <span class="s0">// The recommended global threshold below which results are considered</span>
  <span class="s0">// low-confidence and should be filtered out.</span>
  <span class="s0">global_score_threshold:float;</span>
<span class="s0">}</span>

<span class="s0">// Performs Bert tokenization as in tf.text.BertTokenizer</span>
<span class="s0">// (https://github.com/tensorflow/text/blob/3599f6fcd2b780a2dc413b90fb9315464f10b314/docs/api_docs/python/text/BertTokenizer.md)</span>
<span class="s0">// Added in: 1.1.0</span>
<span class="s0">table BertTokenizerOptions {</span>
  <span class="s0">// The vocabulary files used in the BertTokenizer.</span>
  <span class="s0">vocab_file:[AssociatedFile];</span>
<span class="s0">}</span>

<span class="s0">// Performs SentencePiece tokenization as in tf.text.SentencepieceTokenizer</span>
<span class="s0">// (https://github.com/tensorflow/text/blob/3599f6fcd2b780a2dc413b90fb9315464f10b314/docs/api_docs/python/text/SentencepieceTokenizer.md).</span>
<span class="s0">// Added in: 1.1.0</span>
<span class="s0">table SentencePieceTokenizerOptions {</span>
  <span class="s0">// The SentencePiece model files used in the SentencePieceTokenizer.</span>
  <span class="s0">sentencePiece_model:[AssociatedFile];</span>

  <span class="s0">// The optional vocabulary model files used in the SentencePieceTokenizer.</span>
  <span class="s0">vocab_file:[AssociatedFile];</span>
<span class="s0">}</span>

<span class="s0">// Splits strings by the occurrences of delim_regex_pattern and converts the</span>
<span class="s0">// tokens into ids. For example, given</span>
<span class="s0">//   delim_regex_pattern: &quot;\W+&quot;,</span>
<span class="s0">//   string: &quot;Words, words, words.&quot;,</span>
<span class="s0">// the tokens after split are: &quot;Words&quot;, &quot;words&quot;, &quot;words&quot;, &quot;&quot;.</span>
<span class="s0">// And then the tokens can be converted into ids according to the vocab_file.</span>
<span class="s0">// Added in: 1.2.1</span>
<span class="s0">table RegexTokenizerOptions {</span>
  <span class="s0">delim_regex_pattern:string;</span>
  <span class="s0">// The vocabulary files used to convert this tokens into ids.</span>
  <span class="s0">vocab_file:[AssociatedFile];</span>
<span class="s0">}</span>

<span class="s0">// Options that are used when processing the tensor.</span>
<span class="s0">union ProcessUnitOptions {</span>
  <span class="s0">NormalizationOptions,</span>
  <span class="s0">ScoreCalibrationOptions,</span>
  <span class="s0">ScoreThresholdingOptions,</span>
  <span class="s0">// Added in: 1.1.0</span>
  <span class="s0">BertTokenizerOptions,</span>
  <span class="s0">// Added in: 1.1.0</span>
  <span class="s0">SentencePieceTokenizerOptions,</span>
  <span class="s0">// Added in: 1.2.1</span>
  <span class="s0">RegexTokenizerOptions</span>
<span class="s0">}</span>

<span class="s0">// A process unit that is used to process the tensor out-of-graph.</span>
<span class="s0">table ProcessUnit {</span>
  <span class="s0">options:ProcessUnitOptions;</span>
<span class="s0">}</span>


<span class="s0">// Statistics to describe a tensor.</span>
<span class="s0">table Stats {</span>
  <span class="s0">// Max and min are not currently used in tflite.support codegen. They mainly</span>
  <span class="s0">// serve as references for users to better understand the model. They can also</span>
  <span class="s0">// be used to validate model pre/post processing results.</span>
  <span class="s0">// If there is only one value in max or min, we'll propogate the value to</span>
  <span class="s0">// all channels.</span>

  <span class="s0">// Per-channel maximum value of the tensor.</span>
  <span class="s0">max:[float];</span>

  <span class="s0">// Per-channel minimum value of the tensor.</span>
  <span class="s0">min:[float];</span>
<span class="s0">}</span>

<span class="s0">// Metadata of a group of tensors. It may contain several tensors that will be</span>
<span class="s0">// grouped together in codegen. For example, the TFLite object detection model</span>
<span class="s0">// example (https://www.tensorflow.org/lite/models/object_detection/overview)</span>
<span class="s0">// has four outputs: classes, scores, bounding boxes, and number of detections.</span>
<span class="s0">// If the four outputs are bundled together using TensorGroup (for example,</span>
<span class="s0">// named as &quot;detection result&quot;), the codegen tool will generate the class,</span>
<span class="s0">// `DetectionResult`, which contains the class, score, and bouding box. And the</span>
<span class="s0">// outputs of the model will be converted to a list of `DetectionResults` and</span>
<span class="s0">// the number of detection. Note that the number of detection is a single</span>
<span class="s0">// number, therefore is inappropriate for the list of `DetectionResult`.</span>
<span class="s0">// Added in: 1.2.0</span>
<span class="s0">table TensorGroup {</span>
  <span class="s0">// Name of tensor group.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;codegen usage&gt;:</span>
  <span class="s0">// Name of the joint class of the tensor group.</span>
  <span class="s0">name:string;</span>

  <span class="s0">// Names of the tensors to group together, corresponding to</span>
  <span class="s0">// TensorMetadata.name.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;codegen usage&gt;:</span>
  <span class="s0">// Determines which tensors will be added to this group. All tensors in the</span>
  <span class="s0">// group should have the same number of elements specified by Content.range.</span>
  <span class="s0">tensor_names:[string];</span>
<span class="s0">}</span>

<span class="s0">// Detailed information of an input or output tensor.</span>
<span class="s0">table TensorMetadata {</span>
  <span class="s0">// Name of the tensor.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// The name of this tensor in the generated model interface.</span>
  <span class="s0">name:string;</span>

  <span class="s0">// A description of the tensor.</span>
  <span class="s0">description:string;</span>

  <span class="s0">// A list of names of the dimensions in this tensor. The length of</span>
  <span class="s0">// dimension_names need to match the number of dimensions in this tensor.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// The name of each dimension in the generated model interface. See &quot;Case 2&quot;</span>
  <span class="s0">// in the comments of Content.range.</span>
  <span class="s0">dimension_names:[string];</span>

  <span class="s0">// The content that represents this tensor.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Determines how to process this tensor. See each item in ContentProperties</span>
  <span class="s0">// for the default process units that will be applied to the tensor.</span>
  <span class="s0">content:Content;</span>

  <span class="s0">// The process units that are used to process the tensor out-of-graph.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Contains the parameters of the default processing pipeline for each content</span>
  <span class="s0">// type, such as the normalization parameters in all content types. See the</span>
  <span class="s0">// items under ContentProperties for the details of the default processing</span>
  <span class="s0">// pipeline.</span>
  <span class="s0">process_units:[ProcessUnit];</span>

  <span class="s0">// The statistics of the tensor values.</span>
  <span class="s0">stats:Stats;</span>

  <span class="s0">// A list of associated files of this tensor.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Contains processing parameters of this tensor, such as normalization.</span>
  <span class="s0">associated_files:[AssociatedFile];</span>
<span class="s0">}</span>

<span class="s0">table SubGraphMetadata {</span>
  <span class="s0">// Name of the subgraph.</span>
  <span class="s0">//</span>
  <span class="s0">// Note that, since TFLite only support one subgraph at this moment, the</span>
  <span class="s0">// Codegen tool will use the name in ModelMetadata in the generated model</span>
  <span class="s0">// interface.</span>
  <span class="s0">name:string;</span>

  <span class="s0">// A description explains details about what the subgraph does.</span>
  <span class="s0">description:string;</span>

  <span class="s0">// Metadata of all input tensors used in this subgraph. It matches extactly</span>
  <span class="s0">// the input tensors specified by `SubGraph.inputs` in the TFLite</span>
  <span class="s0">// schema.fbs file[2]. The number of `TensorMetadata` in the array should</span>
  <span class="s0">// equal to the number of indices in `SubGraph.inputs`.</span>
  <span class="s0">//</span>
  <span class="s0">// [2]: tensorflow/lite/schema/schema.fbs</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Determines how to process the inputs.</span>
  <span class="s0">input_tensor_metadata:[TensorMetadata];</span>

  <span class="s0">// Metadata of all output tensors used in this subgraph. It matches extactly</span>
  <span class="s0">// the output tensors specified by `SubGraph.outputs` in the TFLite</span>
  <span class="s0">// schema.fbs file[2]. The number of `TensorMetadata` in the array should</span>
  <span class="s0">// equal to the number of indices in `SubGraph.outputs`.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Determines how to process the outputs.</span>
  <span class="s0">output_tensor_metadata:[TensorMetadata];</span>

  <span class="s0">// A list of associated files of this subgraph.</span>
  <span class="s0">associated_files:[AssociatedFile];</span>

  <span class="s0">// Input process units of the subgraph. Some models may have complex pre and</span>
  <span class="s0">// post processing logics where the process units do not work on one tensor at</span>
  <span class="s0">// a time, but in a similar way of a TFLite graph. For example, in the</span>
  <span class="s0">// MobileBert model (https://www.tensorflow.org/lite/models/bert_qa/overview),</span>
  <span class="s0">// the inputs are: ids / mask / segment ids;</span>
  <span class="s0">// the outputs are: end logits / start logits.</span>
  <span class="s0">// The preprocessing converts the query string and the context string to the</span>
  <span class="s0">// model inputs, and the post-processing converts the model outputs to the</span>
  <span class="s0">// answer string.</span>
  <span class="s0">// Added in: 1.1.0</span>
  <span class="s0">input_process_units:[ProcessUnit];</span>

  <span class="s0">// Output process units of the subgraph.</span>
  <span class="s0">// Added in: 1.1.0</span>
  <span class="s0">output_process_units:[ProcessUnit];</span>

  <span class="s0">// Metadata of all input tensor groups used in this subgraph.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;codegen usage&gt;:</span>
  <span class="s0">// Bundles the corresponding elements of the underlying input tensors together</span>
  <span class="s0">// into a class, and converts those individual tensors into a list of the</span>
  <span class="s0">// class objects.</span>
  <span class="s0">// Added in: 1.2.0</span>
  <span class="s0">input_tensor_groups:[TensorGroup];</span>

  <span class="s0">// Metadata of all output tensor groups used in this subgraph.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;codegen usage&gt;:</span>
  <span class="s0">// Bundles the corresponding elements of the underlying output tensors</span>
  <span class="s0">// together into a class, and converts those individual tensors into a list of</span>
  <span class="s0">// the class objects.</span>
  <span class="s0">// Added in: 1.2.0</span>
  <span class="s0">output_tensor_groups:[TensorGroup];</span>

<span class="s0">}</span>

<span class="s0">table ModelMetadata {</span>
  <span class="s0">// Name of the model.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// The name of the model in the generated model interface.</span>
  <span class="s0">name:string;</span>

  <span class="s0">// Model description in schema.</span>
  <span class="s0">description:string;</span>

  <span class="s0">// Version of the model that specified by model creators.</span>
  <span class="s0">version:string;</span>

  <span class="s0">// Noted that, the minimum required TFLite runtime version that the model is</span>
  <span class="s0">// compatible with, has already been added as a metadata entry in tflite</span>
  <span class="s0">// schema. We'll decide later if we want to move it here, and keep it with</span>
  <span class="s0">// other metadata entries.</span>

  <span class="s0">// Metadata of all the subgraphs of the model. The 0th is assumed to be the</span>
  <span class="s0">// main subgraph.</span>
  <span class="s0">//</span>
  <span class="s0">// &lt;Codegen usage&gt;:</span>
  <span class="s0">// Determines how to process the inputs and outputs.</span>
  <span class="s0">subgraph_metadata:[SubGraphMetadata];</span>

  <span class="s0">// The person who creates this model.</span>
  <span class="s0">author:string;</span>

  <span class="s0">// Licenses that may apply to this model.</span>
  <span class="s0">license:string;</span>

  <span class="s0">// A list of associated files of this model.</span>
  <span class="s0">associated_files:[AssociatedFile];</span>

  <span class="s0">// The minimum metadata parser version that can fully understand the fields in</span>
  <span class="s0">// the metadata flatbuffer. The version is effectively the largest version</span>
  <span class="s0">// number among the versions of all the fields populated and the smallest</span>
  <span class="s0">// compatible version indicated by the file identifier.</span>
  <span class="s0">//</span>
  <span class="s0">// This field is automaticaly populated by the MetadataPopulator when</span>
  <span class="s0">// the metadata is populated into a TFLite model.</span>
  <span class="s0">min_parser_version:string;</span>
<span class="s0">}</span>

<span class="s0">root_type ModelMetadata;</span>
</pre>
</body>
</html>