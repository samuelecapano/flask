<html>
<head>
<title>audio_classifier.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
audio_classifier.py</font>
</center></td></tr></table>
<pre><span class="s0"># Copyright 2022 The MediaPipe Authors. All Rights Reserved.</span>
<span class="s0">#</span>
<span class="s0"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="s0"># you may not use this file except in compliance with the License.</span>
<span class="s0"># You may obtain a copy of the License at</span>
<span class="s0">#</span>
<span class="s0">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="s0">#</span>
<span class="s0"># Unless required by applicable law or agreed to in writing, software</span>
<span class="s0"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="s0"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="s0"># See the License for the specific language governing permissions and</span>
<span class="s0"># limitations under the License.</span>
<span class="s2">&quot;&quot;&quot;MediaPipe audio classifier task.&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">dataclasses</span>
<span class="s3">from </span><span class="s1">typing </span><span class="s3">import </span><span class="s1">Callable</span><span class="s3">, </span><span class="s1">Mapping</span><span class="s3">, </span><span class="s1">List</span><span class="s3">, </span><span class="s1">Optional</span>

<span class="s3">from </span><span class="s1">mediapipe.python </span><span class="s3">import </span><span class="s1">packet_creator</span>
<span class="s3">from </span><span class="s1">mediapipe.python </span><span class="s3">import </span><span class="s1">packet_getter</span>
<span class="s3">from </span><span class="s1">mediapipe.python._framework_bindings </span><span class="s3">import </span><span class="s1">packet</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.cc.audio.audio_classifier.proto </span><span class="s3">import </span><span class="s1">audio_classifier_graph_options_pb2</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.cc.components.containers.proto </span><span class="s3">import </span><span class="s1">classifications_pb2</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.cc.components.processors.proto </span><span class="s3">import </span><span class="s1">classifier_options_pb2</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.audio.core </span><span class="s3">import </span><span class="s1">audio_task_running_mode </span><span class="s3">as </span><span class="s1">running_mode_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.audio.core </span><span class="s3">import </span><span class="s1">base_audio_task_api</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.components.containers </span><span class="s3">import </span><span class="s1">audio_data </span><span class="s3">as </span><span class="s1">audio_data_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.components.containers </span><span class="s3">import </span><span class="s1">classification_result </span><span class="s3">as </span><span class="s1">classification_result_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core </span><span class="s3">import </span><span class="s1">base_options </span><span class="s3">as </span><span class="s1">base_options_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core </span><span class="s3">import </span><span class="s1">task_info </span><span class="s3">as </span><span class="s1">task_info_module</span>
<span class="s3">from </span><span class="s1">mediapipe.tasks.python.core.optional_dependencies </span><span class="s3">import </span><span class="s1">doc_controls</span>

<span class="s1">AudioClassifierResult = classification_result_module.ClassificationResult</span>
<span class="s1">_AudioClassifierGraphOptionsProto = audio_classifier_graph_options_pb2.AudioClassifierGraphOptions</span>
<span class="s1">_AudioData = audio_data_module.AudioData</span>
<span class="s1">_BaseOptions = base_options_module.BaseOptions</span>
<span class="s1">_ClassifierOptionsProto = classifier_options_pb2.ClassifierOptions</span>
<span class="s1">_RunningMode = running_mode_module.AudioTaskRunningMode</span>
<span class="s1">_TaskInfo = task_info_module.TaskInfo</span>

<span class="s1">_AUDIO_IN_STREAM_NAME = </span><span class="s4">'audio_in'</span>
<span class="s1">_AUDIO_TAG = </span><span class="s4">'AUDIO'</span>
<span class="s1">_CLASSIFICATIONS_STREAM_NAME = </span><span class="s4">'classifications_out'</span>
<span class="s1">_CLASSIFICATIONS_TAG = </span><span class="s4">'CLASSIFICATIONS'</span>
<span class="s1">_SAMPLE_RATE_IN_STREAM_NAME = </span><span class="s4">'sample_rate_in'</span>
<span class="s1">_SAMPLE_RATE_TAG = </span><span class="s4">'SAMPLE_RATE'</span>
<span class="s1">_TASK_GRAPH_NAME = </span><span class="s4">'mediapipe.tasks.audio.audio_classifier.AudioClassifierGraph'</span>
<span class="s1">_TIMESTAMPED_CLASSIFICATIONS_STREAM_NAME = </span><span class="s4">'timestamped_classifications_out'</span>
<span class="s1">_TIMESTAMPED_CLASSIFICATIONS_TAG = </span><span class="s4">'TIMESTAMPED_CLASSIFICATIONS'</span>
<span class="s1">_MICRO_SECONDS_PER_MILLISECOND = </span><span class="s5">1000</span>


<span class="s1">@dataclasses.dataclass</span>
<span class="s3">class </span><span class="s1">AudioClassifierOptions:</span>
  <span class="s2">&quot;&quot;&quot;Options for the audio classifier task. 
 
  Attributes: 
    base_options: Base options for the audio classifier task. 
    running_mode: The running mode of the task. Default to the audio clips mode. 
      Audio classifier task has two running modes: 1) The audio clips mode for 
      running classification on independent audio clips. 2) The audio stream 
      mode for running classification on the audio stream, such as from 
      microphone. In this mode,  the &quot;result_callback&quot; below must be specified 
      to receive the classification results asynchronously. 
    display_names_locale: The locale to use for display names specified through 
      the TFLite Model Metadata. 
    max_results: The maximum number of top-scored classification results to 
      return. 
    score_threshold: Overrides the ones provided in the model metadata. Results 
      below this value are rejected. 
    category_allowlist: Allowlist of category names. If non-empty, 
      classification results whose category name is not in this set will be 
      filtered out. Duplicate or unknown category names are ignored. Mutually 
      exclusive with `category_denylist`. 
    category_denylist: Denylist of category names. If non-empty, classification 
      results whose category name is in this set will be filtered out. Duplicate 
      or unknown category names are ignored. Mutually exclusive with 
      `category_allowlist`. 
    result_callback: The user-defined result callback for processing audio 
      stream data. The result callback should only be specified when the running 
      mode is set to the audio stream mode. 
  &quot;&quot;&quot;</span>
  <span class="s1">base_options: _BaseOptions</span>
  <span class="s1">running_mode: _RunningMode = _RunningMode.AUDIO_CLIPS</span>
  <span class="s1">display_names_locale: Optional[str] = </span><span class="s3">None</span>
  <span class="s1">max_results: Optional[int] = </span><span class="s3">None</span>
  <span class="s1">score_threshold: Optional[float] = </span><span class="s3">None</span>
  <span class="s1">category_allowlist: Optional[List[str]] = </span><span class="s3">None</span>
  <span class="s1">category_denylist: Optional[List[str]] = </span><span class="s3">None</span>
  <span class="s1">result_callback: Optional[Callable[[AudioClassifierResult</span><span class="s3">, </span><span class="s1">int]</span><span class="s3">, None</span><span class="s1">]] = </span><span class="s3">None</span>

  <span class="s1">@doc_controls.do_not_generate_docs</span>
  <span class="s3">def </span><span class="s1">to_pb2(self) -&gt; _AudioClassifierGraphOptionsProto:</span>
    <span class="s2">&quot;&quot;&quot;Generates an AudioClassifierOptions protobuf object.&quot;&quot;&quot;</span>
    <span class="s1">base_options_proto = self.base_options.to_pb2()</span>
    <span class="s1">base_options_proto.use_stream_mode = </span><span class="s3">False if </span><span class="s1">self.running_mode == _RunningMode.AUDIO_CLIPS </span><span class="s3">else True</span>
    <span class="s1">classifier_options_proto = _ClassifierOptionsProto(</span>
        <span class="s1">score_threshold=self.score_threshold</span><span class="s3">,</span>
        <span class="s1">category_allowlist=self.category_allowlist</span><span class="s3">,</span>
        <span class="s1">category_denylist=self.category_denylist</span><span class="s3">,</span>
        <span class="s1">display_names_locale=self.display_names_locale</span><span class="s3">,</span>
        <span class="s1">max_results=self.max_results)</span>

    <span class="s3">return </span><span class="s1">_AudioClassifierGraphOptionsProto(</span>
        <span class="s1">base_options=base_options_proto</span><span class="s3">,</span>
        <span class="s1">classifier_options=classifier_options_proto)</span>


<span class="s3">class </span><span class="s1">AudioClassifier(base_audio_task_api.BaseAudioTaskApi):</span>
  <span class="s2">&quot;&quot;&quot;Class that performs audio classification on audio data. 
 
  This API expects a TFLite model with mandatory TFLite Model Metadata that 
  contains the mandatory AudioProperties of the solo input audio tensor and the 
  optional (but recommended) category labels as AssociatedFiles with type 
  TENSOR_AXIS_LABELS per output classification tensor. 
 
  Input tensor: 
    (kTfLiteFloat32) 
    - input audio buffer of size `[batch * samples]`. 
    - batch inference is not supported (`batch` is required to be 1). 
    - for multi-channel models, the channels must be interleaved. 
  At least one output tensor with: 
    (kTfLiteFloat32) 
    - `[1 x N]` array with `N` represents the number of categories. 
    - optional (but recommended) category labels as AssociatedFiles with type 
      TENSOR_AXIS_LABELS, containing one label per line. The first such 
      AssociatedFile (if any) is used to fill the `category_name` field of the 
      results. The `display_name` field is filled from the AssociatedFile (if 
      any) whose locale matches the `display_names_locale` field of the 
      `AudioClassifierOptions` used at creation time (&quot;en&quot; by default, i.e. 
      English). If none of these are available, only the `index` field of the 
      results will be filled. 
  &quot;&quot;&quot;</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">create_from_model_path(cls</span><span class="s3">, </span><span class="s1">model_path: str) -&gt; </span><span class="s4">'AudioClassifier'</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Creates an `AudioClassifier` object from a TensorFlow Lite model and the default `AudioClassifierOptions`. 
 
    Note that the created `AudioClassifier` instance is in audio clips mode, for 
    classifying on independent audio clips. 
 
    Args: 
      model_path: Path to the model. 
 
    Returns: 
      `AudioClassifier` object that's created from the model file and the 
      default `AudioClassifierOptions`. 
 
    Raises: 
      ValueError: If failed to create `AudioClassifier` object from the provided 
        file such as invalid file path. 
      RuntimeError: If other types of error occurred. 
    &quot;&quot;&quot;</span>
    <span class="s1">base_options = _BaseOptions(model_asset_path=model_path)</span>
    <span class="s1">options = AudioClassifierOptions(</span>
        <span class="s1">base_options=base_options</span><span class="s3">, </span><span class="s1">running_mode=_RunningMode.AUDIO_CLIPS)</span>
    <span class="s3">return </span><span class="s1">cls.create_from_options(options)</span>

  <span class="s1">@classmethod</span>
  <span class="s3">def </span><span class="s1">create_from_options(cls</span><span class="s3">,</span>
                          <span class="s1">options: AudioClassifierOptions) -&gt; </span><span class="s4">'AudioClassifier'</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Creates the `AudioClassifier` object from audio classifier options. 
 
    Args: 
      options: Options for the audio classifier task. 
 
    Returns: 
      `AudioClassifier` object that's created from `options`. 
 
    Raises: 
      ValueError: If failed to create `AudioClassifier` object from 
        `AudioClassifierOptions` such as missing the model. 
      RuntimeError: If other types of error occurred. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">packets_callback(output_packets: Mapping[str</span><span class="s3">, </span><span class="s1">packet.Packet]):</span>
      <span class="s1">timestamp_ms = output_packets[</span>
          <span class="s1">_CLASSIFICATIONS_STREAM_NAME].timestamp.value // _MICRO_SECONDS_PER_MILLISECOND</span>
      <span class="s3">if </span><span class="s1">output_packets[_CLASSIFICATIONS_STREAM_NAME].is_empty():</span>
        <span class="s1">options.result_callback(</span>
            <span class="s1">AudioClassifierResult(classifications=[])</span><span class="s3">, </span><span class="s1">timestamp_ms)</span>
        <span class="s3">return</span>
      <span class="s1">classification_result_proto = classifications_pb2.ClassificationResult()</span>
      <span class="s1">classification_result_proto.CopyFrom(</span>
          <span class="s1">packet_getter.get_proto(output_packets[_CLASSIFICATIONS_STREAM_NAME]))</span>
      <span class="s1">options.result_callback(</span>
          <span class="s1">AudioClassifierResult.create_from_pb2(classification_result_proto)</span><span class="s3">,</span>
          <span class="s1">timestamp_ms)</span>

    <span class="s1">task_info = _TaskInfo(</span>
        <span class="s1">task_graph=_TASK_GRAPH_NAME</span><span class="s3">,</span>
        <span class="s1">input_streams=[</span>
            <span class="s4">':'</span><span class="s1">.join([_AUDIO_TAG</span><span class="s3">, </span><span class="s1">_AUDIO_IN_STREAM_NAME])</span><span class="s3">,</span>
            <span class="s4">':'</span><span class="s1">.join([_SAMPLE_RATE_TAG</span><span class="s3">, </span><span class="s1">_SAMPLE_RATE_IN_STREAM_NAME])</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s1">output_streams=[</span>
            <span class="s4">':'</span><span class="s1">.join([_CLASSIFICATIONS_TAG</span><span class="s3">, </span><span class="s1">_CLASSIFICATIONS_STREAM_NAME])</span><span class="s3">,</span>
            <span class="s4">':'</span><span class="s1">.join([</span>
                <span class="s1">_TIMESTAMPED_CLASSIFICATIONS_TAG</span><span class="s3">,</span>
                <span class="s1">_TIMESTAMPED_CLASSIFICATIONS_STREAM_NAME</span>
            <span class="s1">])</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s1">task_options=options)</span>
    <span class="s3">return </span><span class="s1">cls(</span>
        <span class="s0"># Audio tasks should not drop input audio due to flow limiting, which</span>
        <span class="s0"># may cause data inconsistency.</span>
        <span class="s1">task_info.generate_graph_config(enable_flow_limiting=</span><span class="s3">False</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">options.running_mode</span><span class="s3">,</span>
        <span class="s1">packets_callback </span><span class="s3">if </span><span class="s1">options.result_callback </span><span class="s3">else None</span><span class="s1">)</span>

  <span class="s3">def </span><span class="s1">classify(self</span><span class="s3">, </span><span class="s1">audio_clip: _AudioData) -&gt; List[AudioClassifierResult]:</span>
    <span class="s2">&quot;&quot;&quot;Performs audio classification on the provided audio clip. 
 
    The audio clip is represented as a MediaPipe AudioData. The method accepts 
    audio clips with various length and audio sample rate. It's required to 
    provide the corresponding audio sample rate within the `AudioData` object. 
 
    The input audio clip may be longer than what the model is able to process 
    in a single inference. When this occurs, the input audio clip is split into 
    multiple chunks starting at different timestamps. For this reason, this 
    function returns a vector of ClassificationResult objects, each associated 
    ith a timestamp corresponding to the start (in milliseconds) of the chunk 
    data that was classified, e.g: 
 
    ClassificationResult #0 (first chunk of data): 
      timestamp_ms: 0 (starts at 0ms) 
      classifications #0 (single head model): 
        category #0: 
          category_name: &quot;Speech&quot; 
          score: 0.6 
        category #1: 
          category_name: &quot;Music&quot; 
          score: 0.2 
    ClassificationResult #1 (second chunk of data): 
      timestamp_ms: 800 (starts at 800ms) 
      classifications #0 (single head model): 
        category #0: 
          category_name: &quot;Speech&quot; 
          score: 0.5 
       category #1: 
         category_name: &quot;Silence&quot; 
         score: 0.1 
 
    Args: 
      audio_clip: MediaPipe AudioData. 
 
    Returns: 
      An `AudioClassifierResult` object that contains a list of 
      classification result objects, each associated with a timestamp 
      corresponding to the start (in milliseconds) of the chunk data that was 
      classified. 
 
    Raises: 
      ValueError: If any of the input arguments is invalid, such as the sample 
        rate is not provided in the `AudioData` object. 
      RuntimeError: If audio classification failed to run. 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">audio_clip.audio_format.sample_rate:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Must provide the audio sample rate in audio data.'</span><span class="s1">)</span>
    <span class="s1">output_packets = self._process_audio_clip({</span>
        <span class="s1">_AUDIO_IN_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_matrix(audio_clip.buffer</span><span class="s3">, </span><span class="s1">transpose=</span><span class="s3">True</span><span class="s1">)</span><span class="s3">,</span>
        <span class="s1">_SAMPLE_RATE_IN_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_double(audio_clip.audio_format.sample_rate)</span>
    <span class="s1">})</span>
    <span class="s1">output_list = []</span>
    <span class="s1">classification_result_proto_list = packet_getter.get_proto_list(</span>
        <span class="s1">output_packets[_TIMESTAMPED_CLASSIFICATIONS_STREAM_NAME])</span>
    <span class="s3">for </span><span class="s1">proto </span><span class="s3">in </span><span class="s1">classification_result_proto_list:</span>
      <span class="s1">classification_result_proto = classifications_pb2.ClassificationResult()</span>
      <span class="s1">classification_result_proto.CopyFrom(proto)</span>
      <span class="s1">output_list.append(</span>
          <span class="s1">AudioClassifierResult.create_from_pb2(classification_result_proto))</span>
    <span class="s3">return </span><span class="s1">output_list</span>

  <span class="s3">def </span><span class="s1">classify_async(self</span><span class="s3">, </span><span class="s1">audio_block: _AudioData</span><span class="s3">, </span><span class="s1">timestamp_ms: int) -&gt; </span><span class="s3">None</span><span class="s1">:</span>
    <span class="s2">&quot;&quot;&quot;Sends audio data (a block in a continuous audio stream) to perform audio classification. 
 
    Only use this method when the AudioClassifier is created with the audio 
    stream running mode. The input timestamps should be monotonically increasing 
    for adjacent calls of this method. This method will return immediately after 
    the input audio data is accepted. The results will be available via the 
    `result_callback` provided in the `AudioClassifierOptions`. The 
    `classify_async` method is designed to process auido stream data such as 
    microphone input. 
 
    The input audio data may be longer than what the model is able to process 
    in a single inference. When this occurs, the input audio block is split 
    into multiple chunks. For this reason, the callback may be called multiple 
    times (once per chunk) for each call to this function. 
 
    The `result_callback` provides: 
      - An `AudioClassifierResult` object that contains a list of 
        classifications. 
      - The input timestamp in milliseconds. 
 
    Args: 
      audio_block: MediaPipe AudioData. 
      timestamp_ms: The timestamp of the input audio data in milliseconds. 
 
    Raises: 
      ValueError: If any of the followings: 
        1) The sample rate is not provided in the `AudioData` object or the 
        provided sample rate is inconsistent with the previously received. 
        2) The current input timestamp is smaller than what the audio 
        classifier has already processed. 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">audio_block.audio_format.sample_rate:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">'Must provide the audio sample rate in audio data.'</span><span class="s1">)</span>
    <span class="s3">if not </span><span class="s1">self._default_sample_rate:</span>
      <span class="s1">self._default_sample_rate = audio_block.audio_format.sample_rate</span>
      <span class="s1">self._set_sample_rate(_SAMPLE_RATE_IN_STREAM_NAME</span><span class="s3">,</span>
                            <span class="s1">self._default_sample_rate)</span>
    <span class="s3">elif </span><span class="s1">audio_block.audio_format.sample_rate != self._default_sample_rate:</span>
      <span class="s3">raise </span><span class="s1">ValueError(</span>
          <span class="s4">f'The audio sample rate provided in audio data: '</span>
          <span class="s4">f'</span><span class="s3">{</span><span class="s1">audio_block.audio_format.sample_rate</span><span class="s3">} </span><span class="s4">is inconsistent with '</span>
          <span class="s4">f'the previously received: </span><span class="s3">{</span><span class="s1">self._default_sample_rate</span><span class="s3">}</span><span class="s4">.'</span><span class="s1">)</span>

    <span class="s1">self._send_audio_stream_data({</span>
        <span class="s1">_AUDIO_IN_STREAM_NAME:</span>
            <span class="s1">packet_creator.create_matrix(audio_block.buffer</span><span class="s3">, </span><span class="s1">transpose=</span><span class="s3">True</span><span class="s1">).at(</span>
                <span class="s1">timestamp_ms * _MICRO_SECONDS_PER_MILLISECOND)</span>
    <span class="s1">})</span>
</pre>
</body>
</html>